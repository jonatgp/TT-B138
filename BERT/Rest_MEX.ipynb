{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "contador_llamadas=0\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def mostrar_arboles(text):\n",
    "    global contador_llamadas\n",
    "    \"\"\"\n",
    "    Muestra el árbol sintáctico de la frase original y luego el de la frase\n",
    "    sin preposiciones, artículos, conjunciones y pronombres.\n",
    "\n",
    "    Parámetros:\n",
    "    - text (str): La frase de entrada.\n",
    "\n",
    "    Retorno:\n",
    "    - str: La frase sin preposiciones, artículos, conjunciones y pronombres.\n",
    "    \"\"\"\n",
    "    # Analizar la frase original\n",
    "    doc = nlp(text)\n",
    "\n",
    "\n",
    "    # Filtrar palabras que no son preposiciones, artículos, conjunciones ni pronombres\n",
    "    filtered_words = [\n",
    "        token for token in doc\n",
    "        if token.pos_ not in {\"ADP\", \"DET\", \"CCONJ\", \"SCONJ\", \"PRON\"}\n",
    "    ]\n",
    "    # Reconstruir la frase sin estas palabras\n",
    "    filtered_sentence = \" \".join([token.text for token in filtered_words])\n",
    "    # Crear un nuevo Doc a partir de las palabras filtradas\n",
    "    filtered_sentence = re.sub(r'\\S*@\\S*\\s?', '', filtered_sentence)\n",
    "    contador_llamadas += 1\n",
    "    print(contador_llamadas)\n",
    "\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Nombre de carpeta y archivo\n",
    "nombredecarpetaatrabajar = \"REST_MEX/\"\n",
    "nombredearchivo = \"Rest_Mex_Sentiment_Analysis_2023_Train.xlsx\"\n",
    "nombre_archivo = 'CORPUS/' + nombredecarpetaatrabajar + nombredearchivo\n",
    "\n",
    "# Cargar datos desde el archivo CSV\n",
    "full_data = pd.read_excel(nombre_archivo)  # Usamos read_csv porque es un archivo CSV\n",
    "print(full_data)\n",
    "# Convertir todos los valores de 'content' a string\n",
    "full_data['lemma'] = full_data['TituloyResena'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Contar filas antes de eliminar NaN\n",
    "antes = len(full_data)\n",
    "\n",
    "# Reemplazar los valores en la columna 'Polarity'\n",
    "full_data['Polarity'] = full_data['Polarity'].replace({1: 0, 2: 0, 3 : np.nan, 4: 1, 5: 1})\n",
    "\n",
    "# Eliminar filas con NaN en la columna 'Polarity'\n",
    "full_data.dropna(subset=['Polarity'], inplace=True)\n",
    "\n",
    "print(full_data)\n",
    "\n",
    "# Contar filas después de eliminar NaN\n",
    "despues = len(full_data)\n",
    "\n",
    "print(\"Número de filas antes de eliminar NaN:\", antes)\n",
    "print(\"Número de filas después de eliminar NaN:\", despues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.to_pickle('CORPUS/'+nombredecarpetaatrabajar+'/Dataset.pkl')\n",
    "full_data = pd.read_pickle('CORPUS/'+nombredecarpetaatrabajar+'/Dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponible: True\n",
      "Nombre del dispositivo CUDA: NVIDIA GeForce RTX 2060 SUPER\n",
      "Versión de CUDA: 11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Nombre del dispositivo CUDA:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Versión de CUDA:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor en dispositivo: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "try:\n",
    "    tensor = torch.tensor([1.0, 2.0, 3.0]).to(device)\n",
    "    print(\"Tensor en dispositivo:\", tensor.device)\n",
    "except Exception as e:\n",
    "    print(\"Error al mover tensor a la GPU:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS/IMDB/IMDBDatasetSPANISH.csv\n",
      "       Unnamed: 0                                          review_en  \\\n",
      "0               0  One of the other reviewers has mentioned that ...   \n",
      "1               1  A wonderful little production. The filming tec...   \n",
      "2               2  I thought this was a wonderful way to spend ti...   \n",
      "3               3  Basically there's a family where a little boy ...   \n",
      "4               4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
      "...           ...                                                ...   \n",
      "49995       49995  I thought this movie did a down right good job...   \n",
      "49996       49996  Bad plot, bad dialogue, bad acting, idiotic di...   \n",
      "49997       49997  I am a Catholic taught in parochial elementary...   \n",
      "49998       49998  I'm going to have to disagree with the previou...   \n",
      "49999       49999  No one expects the Star Trek movies to be high...   \n",
      "\n",
      "                                               review_es sentiment  \\\n",
      "0      Uno de los otros críticos ha mencionado que de...  positive   \n",
      "1      Una pequeña pequeña producción.La técnica de f...  positive   \n",
      "2      Pensé que esta era una manera maravillosa de p...  positive   \n",
      "3      Básicamente, hay una familia donde un niño peq...  negative   \n",
      "4      El \"amor en el tiempo\" de Petter Mattei es una...  positive   \n",
      "...                                                  ...       ...   \n",
      "49995  Pensé que esta película hizo un buen trabajo a...  positive   \n",
      "49996  Mala parcela, mal diálogo, mala actuación, dir...  negative   \n",
      "49997  Soy católica enseñada en escuelas primarias pa...  negative   \n",
      "49998  Voy a tener que estar en desacuerdo con el com...  negative   \n",
      "49999  Nadie espera que las películas de Star Trek se...  negative   \n",
      "\n",
      "       sentimiento                                              lemma  \n",
      "0                1  Uno de los otros críticos ha mencionado que de...  \n",
      "1                1  Una pequeña pequeña producción.La técnica de f...  \n",
      "2                1  Pensé que esta era una manera maravillosa de p...  \n",
      "3                0  Básicamente, hay una familia donde un niño peq...  \n",
      "4                1  El \"amor en el tiempo\" de Petter Mattei es una...  \n",
      "...            ...                                                ...  \n",
      "49995            1  Pensé que esta película hizo un buen trabajo a...  \n",
      "49996            0  Mala parcela, mal diálogo, mala actuación, dir...  \n",
      "49997            0  Soy católica enseñada en escuelas primarias pa...  \n",
      "49998            0  Voy a tener que estar en desacuerdo con el com...  \n",
      "49999            0  Nadie espera que las películas de Star Trek se...  \n",
      "\n",
      "[50000 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GpJonat\\AppData\\Local\\Temp\\ipykernel_17616\\567952050.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['sentimiento'] = data['sentimiento'].astype(int)  # Asegurarse de que las etiquetas sean enteras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas únicas: [1 0]\n",
      "Número de etiquetas únicas: 2\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4061206de18549ba8299a147b7d5c7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb80694ecf14423b622ef3e3adb9d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GpJonat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8953c9135dd44d959021b1a7f26df959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4133, 'grad_norm': 4.505579471588135, 'learning_rate': 1.96024e-05, 'epoch': 0.1}\n",
      "{'loss': 0.3591, 'grad_norm': 40.17921447753906, 'learning_rate': 1.92024e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3274, 'grad_norm': 12.983050346374512, 'learning_rate': 1.88024e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3371, 'grad_norm': 0.5508607625961304, 'learning_rate': 1.8402400000000002e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3433, 'grad_norm': 9.806771278381348, 'learning_rate': 1.8002400000000002e-05, 'epoch': 0.5}\n",
      "{'loss': 0.319, 'grad_norm': 29.978900909423828, 'learning_rate': 1.76032e-05, 'epoch': 0.6}\n",
      "{'loss': 0.3006, 'grad_norm': 17.30762481689453, 'learning_rate': 1.72032e-05, 'epoch': 0.7}\n",
      "{'loss': 0.2975, 'grad_norm': 18.6928768157959, 'learning_rate': 1.6803200000000002e-05, 'epoch': 0.8}\n",
      "{'loss': 0.3139, 'grad_norm': 7.573062419891357, 'learning_rate': 1.6403200000000002e-05, 'epoch': 0.9}\n",
      "{'loss': 0.3053, 'grad_norm': 11.35680103302002, 'learning_rate': 1.60032e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d06a22a27df465aa6b87c60ca723647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36566928029060364, 'eval_runtime': 36.3426, 'eval_samples_per_second': 275.159, 'eval_steps_per_second': 34.395, 'epoch': 1.0}\n",
      "{'loss': 0.211, 'grad_norm': 0.12803401052951813, 'learning_rate': 1.5604000000000002e-05, 'epoch': 1.1}\n",
      "{'loss': 0.2074, 'grad_norm': 2.165412187576294, 'learning_rate': 1.5204e-05, 'epoch': 1.2}\n",
      "{'loss': 0.2105, 'grad_norm': 0.5245097875595093, 'learning_rate': 1.4804000000000001e-05, 'epoch': 1.3}\n",
      "{'loss': 0.2307, 'grad_norm': 0.06689663231372833, 'learning_rate': 1.4404800000000001e-05, 'epoch': 1.4}\n",
      "{'loss': 0.2237, 'grad_norm': 0.2613588273525238, 'learning_rate': 1.40048e-05, 'epoch': 1.5}\n",
      "{'loss': 0.2302, 'grad_norm': 5.528226852416992, 'learning_rate': 1.36048e-05, 'epoch': 1.6}\n",
      "{'loss': 0.1981, 'grad_norm': 14.446507453918457, 'learning_rate': 1.3204800000000003e-05, 'epoch': 1.7}\n",
      "{'loss': 0.2337, 'grad_norm': 8.71195125579834, 'learning_rate': 1.2804800000000001e-05, 'epoch': 1.8}\n",
      "{'loss': 0.2333, 'grad_norm': 17.01825714111328, 'learning_rate': 1.2404800000000002e-05, 'epoch': 1.9}\n",
      "{'loss': 0.2252, 'grad_norm': 0.08580455183982849, 'learning_rate': 1.20048e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7023f235ec43b3be11ca4b1d74d041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3394699692726135, 'eval_runtime': 36.8794, 'eval_samples_per_second': 271.154, 'eval_steps_per_second': 33.894, 'epoch': 2.0}\n",
      "{'loss': 0.117, 'grad_norm': 0.06148986890912056, 'learning_rate': 1.1605600000000001e-05, 'epoch': 2.1}\n",
      "{'loss': 0.1174, 'grad_norm': 1.4835909605026245, 'learning_rate': 1.1205600000000002e-05, 'epoch': 2.2}\n",
      "{'loss': 0.1204, 'grad_norm': 0.03757534548640251, 'learning_rate': 1.08056e-05, 'epoch': 2.3}\n",
      "{'loss': 0.1349, 'grad_norm': 0.05572096258401871, 'learning_rate': 1.04056e-05, 'epoch': 2.4}\n",
      "{'loss': 0.1362, 'grad_norm': 28.937820434570312, 'learning_rate': 1.00056e-05, 'epoch': 2.5}\n",
      "{'loss': 0.1112, 'grad_norm': 0.020086931064724922, 'learning_rate': 9.606400000000002e-06, 'epoch': 2.6}\n",
      "{'loss': 0.132, 'grad_norm': 0.034443192183971405, 'learning_rate': 9.2064e-06, 'epoch': 2.7}\n",
      "{'loss': 0.1278, 'grad_norm': 0.03326597064733505, 'learning_rate': 8.807200000000001e-06, 'epoch': 2.8}\n",
      "{'loss': 0.1357, 'grad_norm': 0.21093279123306274, 'learning_rate': 8.407200000000001e-06, 'epoch': 2.9}\n",
      "{'loss': 0.1054, 'grad_norm': 0.06044214591383934, 'learning_rate': 8.0072e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a9c171e9f94637a893b83b901adaa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.49504658579826355, 'eval_runtime': 36.2395, 'eval_samples_per_second': 275.942, 'eval_steps_per_second': 34.493, 'epoch': 3.0}\n",
      "{'loss': 0.0549, 'grad_norm': 0.009915894828736782, 'learning_rate': 7.6072e-06, 'epoch': 3.1}\n",
      "{'loss': 0.0444, 'grad_norm': 0.0093997186049819, 'learning_rate': 7.207200000000001e-06, 'epoch': 3.2}\n",
      "{'loss': 0.0645, 'grad_norm': 0.8453999161720276, 'learning_rate': 6.807200000000001e-06, 'epoch': 3.3}\n",
      "{'loss': 0.0457, 'grad_norm': 0.006219496950507164, 'learning_rate': 6.4072e-06, 'epoch': 3.4}\n",
      "{'loss': 0.0539, 'grad_norm': 34.527679443359375, 'learning_rate': 6.007200000000001e-06, 'epoch': 3.5}\n",
      "{'loss': 0.0586, 'grad_norm': 0.010505067184567451, 'learning_rate': 5.608e-06, 'epoch': 3.6}\n",
      "{'loss': 0.0416, 'grad_norm': 0.12293388694524765, 'learning_rate': 5.208000000000001e-06, 'epoch': 3.7}\n",
      "{'loss': 0.0647, 'grad_norm': 0.0144659960642457, 'learning_rate': 4.8088000000000004e-06, 'epoch': 3.8}\n",
      "{'loss': 0.0506, 'grad_norm': 0.014976494945585728, 'learning_rate': 4.4088e-06, 'epoch': 3.9}\n",
      "{'loss': 0.0591, 'grad_norm': 0.009728442877531052, 'learning_rate': 4.0088000000000005e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5dfd64a82224bcfbbf06acb11cb8a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5410951375961304, 'eval_runtime': 34.1146, 'eval_samples_per_second': 293.13, 'eval_steps_per_second': 36.641, 'epoch': 4.0}\n",
      "{'loss': 0.0223, 'grad_norm': 0.12309235334396362, 'learning_rate': 3.6088e-06, 'epoch': 4.1}\n",
      "{'loss': 0.0179, 'grad_norm': 0.0036281610373407602, 'learning_rate': 3.2096000000000006e-06, 'epoch': 4.2}\n",
      "{'loss': 0.017, 'grad_norm': 0.004299131687730551, 'learning_rate': 2.8096e-06, 'epoch': 4.3}\n",
      "{'loss': 0.0284, 'grad_norm': 0.004546491429209709, 'learning_rate': 2.4096e-06, 'epoch': 4.4}\n",
      "{'loss': 0.0249, 'grad_norm': 0.07431141287088394, 'learning_rate': 2.0096e-06, 'epoch': 4.5}\n",
      "{'loss': 0.0235, 'grad_norm': 0.00965035054832697, 'learning_rate': 1.6096e-06, 'epoch': 4.6}\n",
      "{'loss': 0.0298, 'grad_norm': 0.004322187975049019, 'learning_rate': 1.2096e-06, 'epoch': 4.7}\n",
      "{'loss': 0.0158, 'grad_norm': 0.0035967326257377863, 'learning_rate': 8.096000000000002e-07, 'epoch': 4.8}\n",
      "{'loss': 0.0193, 'grad_norm': 0.0032111613545566797, 'learning_rate': 4.0960000000000007e-07, 'epoch': 4.9}\n",
      "{'loss': 0.0262, 'grad_norm': 0.09526637941598892, 'learning_rate': 1.04e-08, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c87096ca954c618576902f529935da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.632770299911499, 'eval_runtime': 34.001, 'eval_samples_per_second': 294.109, 'eval_steps_per_second': 36.764, 'epoch': 5.0}\n",
      "{'train_runtime': 3256.703, 'train_samples_per_second': 61.412, 'train_steps_per_second': 7.676, 'train_loss': 0.15042834924697876, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eba7602d05f4dbca31cc2f9ac863ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91      4961\n",
      "           1       0.92      0.91      0.92      5039\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.91      0.92      0.91     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n",
      "Guardando el modelo y el tokenizador en: IMDB//constopwords//model\n",
      "Modelo y tokenizador guardados exitosamente.\n",
      "Cargando el modelo y el tokenizador desde: IMDB//constopwords//model\n",
      "Texto: Este producto es excelente! -> Clase Predicha: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Definir nombres de archivo\n",
    "nombredecarpetaatrabajar = \"REST_MEX/\"\n",
    "nombredearchivo = \"Rest_Mex_Sentiment_Analysis_2023_Train\"\n",
    "nombre_archivo = 'CORPUS/' + nombredecarpetaatrabajar + nombredearchivo\n",
    "print(nombre_archivo)\n",
    "\n",
    "# Cargar el DataFrame\n",
    "full_data =  pd.read_pickle('CORPUS/'+nombredecarpetaatrabajar+'/Dataset.pkl')\n",
    "print(full_data)\n",
    "\n",
    "# Seleccionar las columnas necesarias\n",
    "data = full_data[['lemma', 'sentimiento']]\n",
    "data['sentimiento'] = data['sentimiento'].astype(int)  # Asegurarse de que las etiquetas sean enteras\n",
    "\n",
    "unique_labels = data['sentimiento'].unique()\n",
    "print(\"Etiquetas únicas:\", unique_labels)\n",
    "print(\"Número de etiquetas únicas:\", len(unique_labels))\n",
    "\n",
    "# Dividir el DataFrame en conjunto de entrenamiento (80%) y prueba (20%)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verificar si CUDA está disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Cargar el tokenizador y el modelo BETO\n",
    "model_name = \"dccuchile/bert-base-spanish-wwm-cased\"  # BETO\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Cambia num_labels según tus clases\n",
    "\n",
    "# Mover el modelo a la GPU si está disponible\n",
    "model.to(device)\n",
    "\n",
    "# Convertir los DataFrames a conjuntos de full_data de Hugging Face\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "# Tokenización\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(examples['lemma'], truncation=True, padding='max_length', max_length=256)\n",
    "    tokenized['labels'] = examples['sentimiento']  # Agregar etiquetas\n",
    "    return tokenized\n",
    "\n",
    "# Aplicar tokenización\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Configuración de los argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,  # Reducido para ahorrar memoria\n",
    "    per_device_eval_batch_size=8,    # Reducido para ahorrar memoria\n",
    "    num_train_epochs=5,               # Ajustar según necesidad\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,                        # Habilitar FP16 para ahorro de memoria\n",
    ")\n",
    "\n",
    "# Definir el entrenador\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "predictions = trainer.predict(tokenized_test)\n",
    "predicted_classes = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Convertir las etiquetas verdaderas a un formato numpy\n",
    "y_test_np = np.array(test_data['sentimiento'])\n",
    "\n",
    "# Generar el informe de clasificación\n",
    "report = classification_report(y_test_np, predicted_classes, zero_division=0)\n",
    "print(report)\n",
    "\n",
    "# Guardar el modelo\n",
    "output_dir = nombredecarpetaatrabajar+\"/constopwords/\"+\"/model\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Crear el directorio si no existe\n",
    "print(\"Guardando el modelo y el tokenizador en:\", output_dir)\n",
    "model.save_pretrained(output_dir, safe_serialization=False)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"Modelo y tokenizador guardados exitosamente.\")\n",
    "\n",
    "# Cargar el modelo y el tokenizador para verificar\n",
    "print(\"Cargando el modelo y el tokenizador desde:\", output_dir)\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "loaded_model = BertForSequenceClassification.from_pretrained(output_dir)\n",
    "\n",
    "# Verificar si CUDA está disponible y mover el modelo a la GPU si es posible\n",
    "loaded_model.to(device)\n",
    "\n",
    "# Función para predecir el sentimiento de un texto\n",
    "def predict_sentiment(text):\n",
    "    inputs = loaded_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Mover los inputs a la GPU\n",
    "    with torch.no_grad():  # Desactivar el cálculo de gradientes\n",
    "        outputs = loaded_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "    return predicted_class\n",
    "\n",
    "# Probar con un ejemplo\n",
    "text_example = \"Este producto es excelente!\"\n",
    "predicted_class = predict_sentiment(text_example)\n",
    "print(f\"Texto: {text_example} -> Clase Predicha: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generar la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test_np, predicted_classes)\n",
    "\n",
    "# Etiquetas personalizadas\n",
    "labels_x = [\"Falsos Positivos (FV)\", \"Verdaderos Positivos (VV)\"]\n",
    "labels_y = [\"Falsos Negativos (VF)\", \"Verdaderos Negativos (FF)\"]\n",
    "\n",
    "# Visualizar la matriz de confusión\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels_x, yticklabels=labels_y)\n",
    "plt.title('Matriz de Confusión con Etiquetas Descriptivas')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Valores Reales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# Cargar el modelo y el tokenizador para verificar\n",
    "nombredecarpetaatrabajar = \"IMDB/\"\n",
    "nombredearchivo = \"IMDBDatasetSPANISH.csv\"\n",
    "nombre_archivo = 'CORPUS/' + nombredecarpetaatrabajar + nombredearchivo\n",
    "output_dir = nombredecarpetaatrabajar + \"/constopwords/\" + \"/model\"\n",
    "print(\"Cargando el modelo y el tokenizador desde:\", output_dir)\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "loaded_model = BertForSequenceClassification.from_pretrained(output_dir)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Verificar si CUDA está disponible y mover el modelo a la GPU si es posible\n",
    "loaded_model.to(device)\n",
    "\n",
    "# Función para predecir el sentimiento de un texto\n",
    "def predict_sentiment(text):\n",
    "    inputs = loaded_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Mover los inputs a la GPU\n",
    "    with torch.no_grad():  # Desactivar el cálculo de gradientes\n",
    "        outputs = loaded_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "    return predicted_class\n",
    "\n",
    "# Modificar la función para devolver probabilidades para LIME\n",
    "def predict_proba(texts):\n",
    "    inputs = loaded_tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Mover los inputs a la GPU\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=-1).cpu().numpy()  # Convertir a numpy para LIME\n",
    "    return probabilities\n",
    "\n",
    "# Probar con un ejemplo\n",
    "text_example = \"Es fantástico cuando una película comica no te hace reir.¡¡Qué pena!!Esta película es muy aburrida y larga.Es simplemente doloroso.La historia es da pena y no es divertido. Te sientes mejor cuando está termina\"\n",
    "predicted_class = predict_sentiment(text_example)\n",
    "print(f\"Texto: {text_example} -> Clase Predicha: {predicted_class}\")\n",
    "\n",
    "# Generar explicación con LIME\n",
    "class_names = [\"Negativo\", \"Positivo\"]  # Cambiar según las clases de tu modelo\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "# Crear explicación para un texto\n",
    "exp = explainer.explain_instance(\n",
    "    text_example,                # Texto de ejemplo\n",
    "    predict_proba,               # Función de predicción\n",
    "    num_features=10,             # Número de palabras más importantes a mostrar\n",
    ")\n",
    "\n",
    "# Mostrar resultados en un notebook\n",
    "exp.show_in_notebook()\n",
    "\n",
    "# Guardar la explicación en un archivo HTML\n",
    "exp.save_to_file('lime_explanation.html')\n",
    "\n",
    "# Mostrar explicación en consola\n",
    "for label in [0, 1]:\n",
    "    print(f\"\\nEtiqueta {class_names[label]}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10780"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase predicha: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# Configurar el dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()  # Liberar memoria\n",
    "\n",
    "# Cargar el modelo y el tokenizador\n",
    "nombredecarpetaatrabajar = \"IMDB/\"\n",
    "output_dir = nombredecarpetaatrabajar + \"/stopwords/\" + \"/model\"\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "loaded_model = BertForSequenceClassification.from_pretrained(output_dir).to(device)\n",
    "\n",
    "# Función para predecir el sentimiento\n",
    "def predict_sentiment(text):\n",
    "    inputs = loaded_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)  # Reduce max_length\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    return torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "# Función para devolver probabilidades para LIME\n",
    "# Función para devolver probabilidades corregida\n",
    "def predict_proba(texts):\n",
    "    batch_size = 8  # Tamaño pequeño para evitar saturar la GPU\n",
    "    probabilities = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = loaded_tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = loaded_model(**inputs)\n",
    "        batch_probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        probabilities.extend(batch_probs)\n",
    "    return np.array(probabilities)  # Convertimos a un numpy array\n",
    "\n",
    "# Ejemplo de texto\n",
    "text_example =\"Originalmente, yo era fanático de Tenacious D por su primer álbum y, naturalmente, escuché algunas canciones de The P.O.D., pero quedé bastante decepcionado. Después de ver la película, mi perspectiva cambió. La película es bastante divertida de principio a fin, y me encontré enganchado a pesar de que la trama era realmente absurda, gracias a las actitudes que KG y Jaybles retratan en la película. Mucho más entretenida y agradable que otras películas que he visto en el cine últimamente, como Saw III (aburrida y lenta) o Casino Royale (demasiado homoerótica), aunque he disfrutado mucho entregas anteriores. Si disfrutaste Borat, seguramente disfrutarás la historia de The Greatest Band on Earth.\"\n",
    "predicted_class = predict_sentiment(text_example)\n",
    "print(f\"Clase predicha: {predicted_class}\")\n",
    "\n",
    "# Explicación con LIME\n",
    "class_names = [\"Negativo\", \"Positivo\"]\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "# Generar la explicación\n",
    "exp = explainer.explain_instance(text_example, predict_proba, num_features=10)\n",
    "exp.save_to_file(\"lime_explanationlmm.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.7104,  4.2124]], device='cuda:0')\n",
      "[[3.622639e-04 9.996377e-01]]\n",
      "Texto original: 'Originalmente, yo era fanático de Tenacious D por su primer álbum y, naturalmente, escuché algunas canciones de The P.O.D., pero quedé bastante decepcionado. Después de ver la película, mi perspectiva cambió. La película es bastante divertida de principio a fin, y me encontré enganchado a pesar de que la trama era realmente absurda, gracias a las actitudes que KG y Jaybles retratan en la película. Mucho más entretenida y agradable que otras películas que he visto en el cine últimamente, como Saw III (aburrida y lenta) o Casino Royale (demasiado homoerótica), aunque he disfrutado mucho entregas anteriores. Si disfrutaste Borat, seguramente disfrutarás la historia de The Greatest Band on Earth.'\n",
      "Predicción original: Clase 1, Probabilidades [3.622639e-04 9.996377e-01]\n",
      "tensor([[-3.6969,  4.1810]], device='cuda:0')\n",
      "[[3.7889535e-04 9.9962103e-01]]\n",
      "[3.7889535e-04 9.9962103e-01]\n",
      "Sin 'Originalmente': Clase 1, Probabilidades [3.7889535e-04 9.9962103e-01], Cambio en probabilidad de clase original: 1.6689300537109375e-05\n",
      "tensor([[-3.7009,  4.1906]], device='cuda:0')\n",
      "[[3.7377080e-04 9.9962616e-01]]\n",
      "[3.7377080e-04 9.9962616e-01]\n",
      "Sin ',': Clase 1, Probabilidades [3.7377080e-04 9.9962616e-01], Cambio en probabilidad de clase original: 1.1563301086425781e-05\n",
      "tensor([[-3.6953,  4.1812]], device='cuda:0')\n",
      "[[3.7942419e-04 9.9962056e-01]]\n",
      "[3.7942419e-04 9.9962056e-01]\n",
      "Sin 'yo': Clase 1, Probabilidades [3.7942419e-04 9.9962056e-01], Cambio en probabilidad de clase original: 1.71661376953125e-05\n",
      "tensor([[-3.6956,  4.1826]], device='cuda:0')\n",
      "[[3.7877291e-04 9.9962115e-01]]\n",
      "[3.7877291e-04 9.9962115e-01]\n",
      "Sin 'era': Clase 1, Probabilidades [3.7877291e-04 9.9962115e-01], Cambio en probabilidad de clase original: 1.6570091247558594e-05\n",
      "tensor([[-3.6971,  4.1884]], device='cuda:0')\n",
      "[[3.760450e-04 9.996239e-01]]\n",
      "[3.760450e-04 9.996239e-01]\n",
      "Sin 'fanático': Clase 1, Probabilidades [3.760450e-04 9.996239e-01], Cambio en probabilidad de clase original: 1.3828277587890625e-05\n",
      "tensor([[-3.6973,  4.1838]], device='cuda:0')\n",
      "[[3.7767863e-04 9.9962234e-01]]\n",
      "[3.7767863e-04 9.9962234e-01]\n",
      "Sin 'de': Clase 1, Probabilidades [3.7767863e-04 9.9962234e-01], Cambio en probabilidad de clase original: 1.537799835205078e-05\n",
      "tensor([[-3.6992,  4.1944]], device='cuda:0')\n",
      "[[3.7298136e-04 9.9962699e-01]]\n",
      "[3.7298136e-04 9.9962699e-01]\n",
      "Sin 'Tenacious': Clase 1, Probabilidades [3.7298136e-04 9.9962699e-01], Cambio en probabilidad de clase original: 1.0728836059570312e-05\n",
      "tensor([[-3.7012,  4.1888]], device='cuda:0')\n",
      "[[3.743525e-04 9.996257e-01]]\n",
      "[3.743525e-04 9.996257e-01]\n",
      "Sin 'D': Clase 1, Probabilidades [3.743525e-04 9.996257e-01], Cambio en probabilidad de clase original: 1.2040138244628906e-05\n",
      "tensor([[-3.6982,  4.1860]], device='cuda:0')\n",
      "[[3.7649376e-04 9.9962354e-01]]\n",
      "[3.7649376e-04 9.9962354e-01]\n",
      "Sin 'por': Clase 1, Probabilidades [3.7649376e-04 9.9962354e-01], Cambio en probabilidad de clase original: 1.4185905456542969e-05\n",
      "tensor([[-3.6961,  4.1832]], device='cuda:0')\n",
      "[[3.783673e-04 9.996216e-01]]\n",
      "[3.783673e-04 9.996216e-01]\n",
      "Sin 'su': Clase 1, Probabilidades [3.783673e-04 9.996216e-01], Cambio en probabilidad de clase original: 1.609325408935547e-05\n",
      "tensor([[-3.6924,  4.1771]], device='cuda:0')\n",
      "[[3.8209025e-04 9.9961793e-01]]\n",
      "[3.8209025e-04 9.9961793e-01]\n",
      "Sin 'primer': Clase 1, Probabilidades [3.8209025e-04 9.9961793e-01], Cambio en probabilidad de clase original: 1.9788742065429688e-05\n",
      "tensor([[-3.6934,  4.1754]], device='cuda:0')\n",
      "[[3.823677e-04 9.996176e-01]]\n",
      "[3.823677e-04 9.996176e-01]\n",
      "Sin 'álbum': Clase 1, Probabilidades [3.823677e-04 9.996176e-01], Cambio en probabilidad de clase original: 2.014636993408203e-05\n",
      "tensor([[-3.6971,  4.1807]], device='cuda:0')\n",
      "[[3.7895748e-04 9.9962103e-01]]\n",
      "[3.7895748e-04 9.9962103e-01]\n",
      "Sin 'y': Clase 1, Probabilidades [3.7895748e-04 9.9962103e-01], Cambio en probabilidad de clase original: 1.6689300537109375e-05\n",
      "tensor([[-3.6987,  4.1844]], device='cuda:0')\n",
      "[[3.7694158e-04 9.9962306e-01]]\n",
      "[3.7694158e-04 9.9962306e-01]\n",
      "Sin ',': Clase 1, Probabilidades [3.7694158e-04 9.9962306e-01], Cambio en probabilidad de clase original: 1.4662742614746094e-05\n",
      "tensor([[-3.6974,  4.1846]], device='cuda:0')\n",
      "[[3.7730145e-04 9.9962270e-01]]\n",
      "[3.7730145e-04 9.9962270e-01]\n",
      "Sin 'naturalmente': Clase 1, Probabilidades [3.7730145e-04 9.9962270e-01], Cambio en probabilidad de clase original: 1.5020370483398438e-05\n",
      "tensor([[-3.6990,  4.1856]], device='cuda:0')\n",
      "[[3.7636564e-04 9.9962366e-01]]\n",
      "[3.7636564e-04 9.9962366e-01]\n",
      "Sin ',': Clase 1, Probabilidades [3.7636564e-04 9.9962366e-01], Cambio en probabilidad de clase original: 1.4066696166992188e-05\n",
      "tensor([[-3.6903,  4.1749]], device='cuda:0')\n",
      "[[3.8372559e-04 9.9961627e-01]]\n",
      "[3.8372559e-04 9.9961627e-01]\n",
      "Sin 'escuché': Clase 1, Probabilidades [3.8372559e-04 9.9961627e-01], Cambio en probabilidad de clase original: 2.1457672119140625e-05\n",
      "tensor([[-3.6989,  4.1859]], device='cuda:0')\n",
      "[[3.7626966e-04 9.9962366e-01]]\n",
      "[3.7626966e-04 9.9962366e-01]\n",
      "Sin 'algunas': Clase 1, Probabilidades [3.7626966e-04 9.9962366e-01], Cambio en probabilidad de clase original: 1.4066696166992188e-05\n",
      "tensor([[-3.6957,  4.1808]], device='cuda:0')\n",
      "[[3.7943359e-04 9.9962056e-01]]\n",
      "[3.7943359e-04 9.9962056e-01]\n",
      "Sin 'canciones': Clase 1, Probabilidades [3.7943359e-04 9.9962056e-01], Cambio en probabilidad de clase original: 1.71661376953125e-05\n",
      "tensor([[-3.6991,  4.1887]], device='cuda:0')\n",
      "[[3.7518886e-04 9.9962485e-01]]\n",
      "[3.7518886e-04 9.9962485e-01]\n",
      "Sin 'de': Clase 1, Probabilidades [3.7518886e-04 9.9962485e-01], Cambio en probabilidad de clase original: 1.2874603271484375e-05\n",
      "tensor([[-3.6990,  4.1869]], device='cuda:0')\n",
      "[[3.7588482e-04 9.9962413e-01]]\n",
      "[3.7588482e-04 9.9962413e-01]\n",
      "Sin 'The': Clase 1, Probabilidades [3.7588482e-04 9.9962413e-01], Cambio en probabilidad de clase original: 1.3589859008789062e-05\n",
      "tensor([[-3.7174,  4.2391]], device='cuda:0')\n",
      "[[3.5025360e-04 9.9964976e-01]]\n",
      "[3.5025360e-04 9.9964976e-01]\n",
      "Sin 'P.O.D.': Clase 1, Probabilidades [3.5025360e-04 9.9964976e-01], Cambio en probabilidad de clase original: -1.2040138244628906e-05\n",
      "tensor([[-3.6987,  4.1860]], device='cuda:0')\n",
      "[[3.7631413e-04 9.9962366e-01]]\n",
      "[3.7631413e-04 9.9962366e-01]\n",
      "Sin ',': Clase 1, Probabilidades [3.7631413e-04 9.9962366e-01], Cambio en probabilidad de clase original: 1.4066696166992188e-05\n",
      "tensor([[-3.7022,  4.1891]], device='cuda:0')\n",
      "[[3.7382112e-04 9.9962616e-01]]\n",
      "[3.7382112e-04 9.9962616e-01]\n",
      "Sin 'pero': Clase 1, Probabilidades [3.7382112e-04 9.9962616e-01], Cambio en probabilidad de clase original: 1.1563301086425781e-05\n",
      "tensor([[-3.7056,  4.2035]], device='cuda:0')\n",
      "[[3.672552e-04 9.996327e-01]]\n",
      "[3.672552e-04 9.996327e-01]\n",
      "Sin 'quedé': Clase 1, Probabilidades [3.672552e-04 9.996327e-01], Cambio en probabilidad de clase original: 5.0067901611328125e-06\n",
      "tensor([[-3.7010,  4.1891]], device='cuda:0')\n",
      "[[3.7428236e-04 9.9962568e-01]]\n",
      "[3.7428236e-04 9.9962568e-01]\n",
      "Sin 'bastante': Clase 1, Probabilidades [3.7428236e-04 9.9962568e-01], Cambio en probabilidad de clase original: 1.2040138244628906e-05\n",
      "tensor([[-3.7038,  4.2224]], device='cuda:0')\n",
      "[[3.6101756e-04 9.9963892e-01]]\n",
      "[3.6101756e-04 9.9963892e-01]\n",
      "Sin 'decepcionado': Clase 1, Probabilidades [3.6101756e-04 9.9963892e-01], Cambio en probabilidad de clase original: -1.1920928955078125e-06\n",
      "tensor([[-3.7018,  4.1749]], device='cuda:0')\n",
      "[[3.793486e-04 9.996207e-01]]\n",
      "[3.793486e-04 9.996207e-01]\n",
      "Sin '.': Clase 1, Probabilidades [3.793486e-04 9.996207e-01], Cambio en probabilidad de clase original: 1.704692840576172e-05\n",
      "tensor([[-3.6960,  4.1795]], device='cuda:0')\n",
      "[[3.797687e-04 9.996202e-01]]\n",
      "[3.797687e-04 9.996202e-01]\n",
      "Sin 'Después': Clase 1, Probabilidades [3.797687e-04 9.996202e-01], Cambio en probabilidad de clase original: 1.7523765563964844e-05\n",
      "tensor([[-3.7005,  4.1928]], device='cuda:0')\n",
      "[[3.731062e-04 9.996269e-01]]\n",
      "[3.731062e-04 9.996269e-01]\n",
      "Sin 'de': Clase 1, Probabilidades [3.731062e-04 9.996269e-01], Cambio en probabilidad de clase original: 1.0848045349121094e-05\n",
      "tensor([[-3.7006,  4.1924]], device='cuda:0')\n",
      "[[3.7320046e-04 9.9962676e-01]]\n",
      "[3.7320046e-04 9.9962676e-01]\n",
      "Sin 'ver': Clase 1, Probabilidades [3.7320046e-04 9.9962676e-01], Cambio en probabilidad de clase original: 1.0967254638671875e-05\n",
      "tensor([[-3.7047,  4.1956]], device='cuda:0')\n",
      "[[3.7050492e-04 9.9962950e-01]]\n",
      "[3.7050492e-04 9.9962950e-01]\n",
      "Sin 'la': Clase 1, Probabilidades [3.7050492e-04 9.9962950e-01], Cambio en probabilidad de clase original: 8.225440979003906e-06\n",
      "tensor([[-3.7102,  4.2033]], device='cuda:0')\n",
      "[[3.6562938e-04 9.9963439e-01]]\n",
      "[3.6562938e-04 9.9963439e-01]\n",
      "Sin 'película': Clase 1, Probabilidades [3.6562938e-04 9.9963439e-01], Cambio en probabilidad de clase original: 3.337860107421875e-06\n",
      "tensor([[-3.7047,  4.1896]], device='cuda:0')\n",
      "[[3.7271957e-04 9.9962723e-01]]\n",
      "[3.7271957e-04 9.9962723e-01]\n",
      "Sin ',': Clase 1, Probabilidades [3.7271957e-04 9.9962723e-01], Cambio en probabilidad de clase original: 1.049041748046875e-05\n",
      "tensor([[-3.7004,  4.1861]], device='cuda:0')\n",
      "[[3.7566238e-04 9.9962437e-01]]\n",
      "[3.7566238e-04 9.9962437e-01]\n",
      "Sin 'mi': Clase 1, Probabilidades [3.7566238e-04 9.9962437e-01], Cambio en probabilidad de clase original: 1.33514404296875e-05\n",
      "tensor([[-3.6928,  4.1845]], device='cuda:0')\n",
      "[[3.7910856e-04 9.9962091e-01]]\n",
      "[3.7910856e-04 9.9962091e-01]\n",
      "Sin 'perspectiva': Clase 1, Probabilidades [3.7910856e-04 9.9962091e-01], Cambio en probabilidad de clase original: 1.6808509826660156e-05\n",
      "tensor([[-3.6990,  4.1933]], device='cuda:0')\n",
      "[[3.7347132e-04 9.9962652e-01]]\n",
      "[3.7347132e-04 9.9962652e-01]\n",
      "Sin 'cambió': Clase 1, Probabilidades [3.7347132e-04 9.9962652e-01], Cambio en probabilidad de clase original: 1.1205673217773438e-05\n",
      "tensor([[-3.7130,  4.2065]], device='cuda:0')\n",
      "[[3.6345248e-04 9.9963653e-01]]\n",
      "[3.6345248e-04 9.9963653e-01]\n",
      "Sin '.': Clase 1, Probabilidades [3.6345248e-04 9.9963653e-01], Cambio en probabilidad de clase original: 1.1920928955078125e-06\n",
      "tensor([[-3.7084,  4.1975]], device='cuda:0')\n",
      "[[3.6843526e-04 9.9963152e-01]]\n",
      "[3.6843526e-04 9.9963152e-01]\n",
      "Sin 'La': Clase 1, Probabilidades [3.6843526e-04 9.9963152e-01], Cambio en probabilidad de clase original: 6.198883056640625e-06\n",
      "tensor([[-3.7104,  4.1896]], device='cuda:0')\n",
      "[[3.7063845e-04 9.9962938e-01]]\n",
      "[3.7063845e-04 9.9962938e-01]\n",
      "Sin 'película': Clase 1, Probabilidades [3.7063845e-04 9.9962938e-01], Cambio en probabilidad de clase original: 8.344650268554688e-06\n",
      "tensor([[-3.7012,  4.1917]], device='cuda:0')\n",
      "[[3.7326026e-04 9.9962676e-01]]\n",
      "[3.7326026e-04 9.9962676e-01]\n",
      "Sin 'es': Clase 1, Probabilidades [3.7326026e-04 9.9962676e-01], Cambio en probabilidad de clase original: 1.0967254638671875e-05\n",
      "tensor([[-3.6989,  4.1915]], device='cuda:0')\n",
      "[[3.741691e-04 9.996258e-01]]\n",
      "[3.741691e-04 9.996258e-01]\n",
      "Sin 'bastante': Clase 1, Probabilidades [3.741691e-04 9.996258e-01], Cambio en probabilidad de clase original: 1.1920928955078125e-05\n",
      "tensor([[-3.7186,  4.2077]], device='cuda:0')\n",
      "[[3.6099693e-04 9.9963903e-01]]\n",
      "[3.6099693e-04 9.9963903e-01]\n",
      "Sin 'divertida': Clase 1, Probabilidades [3.6099693e-04 9.9963903e-01], Cambio en probabilidad de clase original: -1.3113021850585938e-06\n",
      "tensor([[-3.6981,  4.1900]], device='cuda:0')\n",
      "[[3.7503545e-04 9.9962497e-01]]\n",
      "[3.7503545e-04 9.9962497e-01]\n",
      "Sin 'de': Clase 1, Probabilidades [3.7503545e-04 9.9962497e-01], Cambio en probabilidad de clase original: 1.2755393981933594e-05\n",
      "tensor([[-3.7028,  4.1929]], device='cuda:0')\n",
      "[[3.7221942e-04 9.9962771e-01]]\n",
      "[3.7221942e-04 9.9962771e-01]\n",
      "Sin 'principio': Clase 1, Probabilidades [3.7221942e-04 9.9962771e-01], Cambio en probabilidad de clase original: 1.0013580322265625e-05\n",
      "tensor([[-3.7102,  4.2028]], device='cuda:0')\n",
      "[[3.6583023e-04 9.9963415e-01]]\n",
      "[3.6583023e-04 9.9963415e-01]\n",
      "Sin 'a': Clase 1, Probabilidades [3.6583023e-04 9.9963415e-01], Cambio en probabilidad de clase original: 3.5762786865234375e-06\n",
      "tensor([[-3.7058,  4.2030]], device='cuda:0')\n",
      "[[3.6732992e-04 9.9963260e-01]]\n",
      "[3.6732992e-04 9.9963260e-01]\n",
      "Sin 'fin': Clase 1, Probabilidades [3.6732992e-04 9.9963260e-01], Cambio en probabilidad de clase original: 5.125999450683594e-06\n",
      "tensor([[-3.6997,  4.1931]], device='cuda:0')\n",
      "[[3.7329510e-04 9.9962664e-01]]\n",
      "[3.7329510e-04 9.9962664e-01]\n",
      "Sin ',': Clase 1, Probabilidades [3.7329510e-04 9.9962664e-01], Cambio en probabilidad de clase original: 1.1086463928222656e-05\n",
      "tensor([[-3.7037,  4.1979]], device='cuda:0')\n",
      "[[3.7002482e-04 9.9962997e-01]]\n",
      "[3.7002482e-04 9.9962997e-01]\n",
      "Sin 'y': Clase 1, Probabilidades [3.7002482e-04 9.9962997e-01], Cambio en probabilidad de clase original: 7.748603820800781e-06\n",
      "tensor([[-3.7164,  4.2180]], device='cuda:0')\n",
      "[[3.5807054e-04 9.9964190e-01]]\n",
      "[3.5807054e-04 9.9964190e-01]\n",
      "Sin 'me': Clase 1, Probabilidades [3.5807054e-04 9.9964190e-01], Cambio en probabilidad de clase original: -4.172325134277344e-06\n",
      "tensor([[-3.7070,  4.2032]], device='cuda:0')\n",
      "[[3.6685108e-04 9.9963319e-01]]\n",
      "[3.6685108e-04 9.9963319e-01]\n",
      "Sin 'encontré': Clase 1, Probabilidades [3.6685108e-04 9.9963319e-01], Cambio en probabilidad de clase original: 4.5299530029296875e-06\n",
      "tensor([[-3.6350,  4.1085]], device='cuda:0')\n",
      "[[4.3335295e-04 9.9956661e-01]]\n",
      "[4.3335295e-04 9.9956661e-01]\n",
      "Sin 'enganchado': Clase 1, Probabilidades [4.3335295e-04 9.9956661e-01], Cambio en probabilidad de clase original: 7.110834121704102e-05\n",
      "tensor([[-3.7100,  4.2035]], device='cuda:0')\n",
      "[[3.656165e-04 9.996344e-01]]\n",
      "[3.656165e-04 9.996344e-01]\n",
      "Sin 'a': Clase 1, Probabilidades [3.656165e-04 9.996344e-01], Cambio en probabilidad de clase original: 3.337860107421875e-06\n",
      "tensor([[-3.6692,  4.1458]], device='cuda:0')\n",
      "[[4.0346442e-04 9.9959654e-01]]\n",
      "[4.0346442e-04 9.9959654e-01]\n",
      "Sin 'pesar': Clase 1, Probabilidades [4.0346442e-04 9.9959654e-01], Cambio en probabilidad de clase original: 4.118680953979492e-05\n",
      "tensor([[-3.7008,  4.1921]], device='cuda:0')\n",
      "[[3.7324885e-04 9.9962676e-01]]\n",
      "[3.7324885e-04 9.9962676e-01]\n",
      "Sin 'de': Clase 1, Probabilidades [3.7324885e-04 9.9962676e-01], Cambio en probabilidad de clase original: 1.0967254638671875e-05\n",
      "tensor([[-3.7014,  4.1920]], device='cuda:0')\n",
      "[[3.7304038e-04 9.9962699e-01]]\n",
      "[3.7304038e-04 9.9962699e-01]\n",
      "Sin 'que': Clase 1, Probabilidades [3.7304038e-04 9.9962699e-01], Cambio en probabilidad de clase original: 1.0728836059570312e-05\n",
      "tensor([[-3.7012,  4.1961]], device='cuda:0')\n",
      "[[3.715972e-04 9.996284e-01]]\n",
      "[3.715972e-04 9.996284e-01]\n",
      "Sin 'la': Clase 1, Probabilidades [3.715972e-04 9.996284e-01], Cambio en probabilidad de clase original: 9.298324584960938e-06\n",
      "tensor([[-3.6926,  4.1802]], device='cuda:0')\n",
      "[[3.8079385e-04 9.9961913e-01]]\n",
      "[3.8079385e-04 9.9961913e-01]\n",
      "Sin 'trama': Clase 1, Probabilidades [3.8079385e-04 9.9961913e-01], Cambio en probabilidad de clase original: 1.8596649169921875e-05\n",
      "tensor([[-3.6989,  4.1919]], device='cuda:0')\n",
      "[[3.740339e-04 9.996259e-01]]\n",
      "[3.740339e-04 9.996259e-01]\n",
      "Sin 'era': Clase 1, Probabilidades [3.740339e-04 9.996259e-01], Cambio en probabilidad de clase original: 1.1801719665527344e-05\n",
      "tensor([[-3.7084,  4.2064]], device='cuda:0')\n",
      "[[3.6516279e-04 9.9963486e-01]]\n",
      "[3.6516279e-04 9.9963486e-01]\n",
      "Sin 'realmente': Clase 1, Probabilidades [3.6516279e-04 9.9963486e-01], Cambio en probabilidad de clase original: 2.86102294921875e-06\n",
      "tensor([[-3.6856,  4.1805]], device='cuda:0')\n",
      "[[3.833888e-04 9.996166e-01]]\n",
      "[3.833888e-04 9.996166e-01]\n",
      "Sin 'absurda': Clase 1, Probabilidades [3.833888e-04 9.996166e-01], Cambio en probabilidad de clase original: 2.110004425048828e-05\n",
      "tensor([[-3.6997,  4.1901]], device='cuda:0')\n",
      "[[3.7438527e-04 9.9962556e-01]]\n",
      "[3.7438527e-04 9.9962556e-01]\n",
      "Sin ',': Clase 1, Probabilidades [3.7438527e-04 9.9962556e-01], Cambio en probabilidad de clase original: 1.2159347534179688e-05\n",
      "tensor([[-3.7073,  4.1982]], device='cuda:0')\n",
      "[[3.685624e-04 9.996314e-01]]\n",
      "[3.685624e-04 9.996314e-01]\n",
      "Sin 'gracias': Clase 1, Probabilidades [3.685624e-04 9.996314e-01], Cambio en probabilidad de clase original: 6.318092346191406e-06\n",
      "tensor([[-3.7044,  4.2008]], device='cuda:0')\n",
      "[[3.6870225e-04 9.9963129e-01]]\n",
      "[3.6870225e-04 9.9963129e-01]\n",
      "Sin 'a': Clase 1, Probabilidades [3.6870225e-04 9.9963129e-01], Cambio en probabilidad de clase original: 6.4373016357421875e-06\n",
      "tensor([[-3.7024,  4.1971]], device='cuda:0')\n",
      "[[3.7080079e-04 9.9962914e-01]]\n",
      "[3.7080079e-04 9.9962914e-01]\n",
      "Sin 'las': Clase 1, Probabilidades [3.7080079e-04 9.9962914e-01], Cambio en probabilidad de clase original: 8.58306884765625e-06\n",
      "tensor([[-3.6973,  4.2033]], device='cuda:0')\n",
      "[[3.7038059e-04 9.9962962e-01]]\n",
      "[3.7038059e-04 9.9962962e-01]\n",
      "Sin 'actitudes': Clase 1, Probabilidades [3.7038059e-04 9.9962962e-01], Cambio en probabilidad de clase original: 8.106231689453125e-06\n",
      "tensor([[-3.6983,  4.1936]], device='cuda:0')\n",
      "[[3.736266e-04 9.996264e-01]]\n",
      "[3.736266e-04 9.996264e-01]\n",
      "Sin 'que': Clase 1, Probabilidades [3.736266e-04 9.996264e-01], Cambio en probabilidad de clase original: 1.1324882507324219e-05\n",
      "tensor([[-3.6960,  4.1921]], device='cuda:0')\n",
      "[[3.7506493e-04 9.9962497e-01]]\n",
      "[3.7506493e-04 9.9962497e-01]\n",
      "Sin 'KG': Clase 1, Probabilidades [3.7506493e-04 9.9962497e-01], Cambio en probabilidad de clase original: 1.2755393981933594e-05\n",
      "tensor([[-3.7005,  4.1919]], device='cuda:0')\n",
      "[[3.7343925e-04 9.9962652e-01]]\n",
      "[3.7343925e-04 9.9962652e-01]\n",
      "Sin 'y': Clase 1, Probabilidades [3.7343925e-04 9.9962652e-01], Cambio en probabilidad de clase original: 1.1205673217773438e-05\n",
      "tensor([[-3.6880,  4.1880]], device='cuda:0')\n",
      "[[3.796119e-04 9.996203e-01]]\n",
      "[3.796119e-04 9.996203e-01]\n",
      "Sin 'Jaybles': Clase 1, Probabilidades [3.796119e-04 9.996203e-01], Cambio en probabilidad de clase original: 1.7404556274414062e-05\n",
      "tensor([[-3.6848,  4.1797]], device='cuda:0')\n",
      "[[3.8396893e-04 9.9961603e-01]]\n",
      "[3.8396893e-04 9.9961603e-01]\n",
      "Sin 'retratan': Clase 1, Probabilidades [3.8396893e-04 9.9961603e-01], Cambio en probabilidad de clase original: 2.1696090698242188e-05\n",
      "tensor([[-3.7013,  4.1984]], device='cuda:0')\n",
      "[[3.7073420e-04 9.9962926e-01]]\n",
      "[3.7073420e-04 9.9962926e-01]\n",
      "Sin 'en': Clase 1, Probabilidades [3.7073420e-04 9.9962926e-01], Cambio en probabilidad de clase original: 8.463859558105469e-06\n",
      "tensor([[-3.7007,  4.1951]], device='cuda:0')\n",
      "[[3.721623e-04 9.996278e-01]]\n",
      "[3.721623e-04 9.996278e-01]\n",
      "Sin 'la': Clase 1, Probabilidades [3.721623e-04 9.996278e-01], Cambio en probabilidad de clase original: 9.894371032714844e-06\n",
      "tensor([[-3.7087,  4.2023]], device='cuda:0')\n",
      "[[3.6654586e-04 9.9963343e-01]]\n",
      "[3.6654586e-04 9.9963343e-01]\n",
      "Sin 'película': Clase 1, Probabilidades [3.6654586e-04 9.9963343e-01], Cambio en probabilidad de clase original: 4.291534423828125e-06\n",
      "tensor([[-3.6881,  4.1762]], device='cuda:0')\n",
      "[[3.8405677e-04 9.9961591e-01]]\n",
      "[3.8405677e-04 9.9961591e-01]\n",
      "Sin '.': Clase 1, Probabilidades [3.8405677e-04 9.9961591e-01], Cambio en probabilidad de clase original: 2.181529998779297e-05\n",
      "tensor([[-3.7059,  4.1935]], device='cuda:0')\n",
      "[[3.7084342e-04 9.9962914e-01]]\n",
      "[3.7084342e-04 9.9962914e-01]\n",
      "Sin 'Mucho': Clase 1, Probabilidades [3.7084342e-04 9.9962914e-01], Cambio en probabilidad de clase original: 8.58306884765625e-06\n",
      "tensor([[-3.7198,  4.2549]], device='cuda:0')\n",
      "[[3.4396144e-04 9.9965608e-01]]\n",
      "[3.4396144e-04 9.9965608e-01]\n",
      "Sin 'más': Clase 1, Probabilidades [3.4396144e-04 9.9965608e-01], Cambio en probabilidad de clase original: -1.8358230590820312e-05\n",
      "tensor([[-3.7177,  4.2542]], device='cuda:0')\n",
      "[[3.4490615e-04 9.9965513e-01]]\n",
      "[3.4490615e-04 9.9965513e-01]\n",
      "Sin 'entretenida': Clase 1, Probabilidades [3.4490615e-04 9.9965513e-01], Cambio en probabilidad de clase original: -1.7404556274414062e-05\n",
      "tensor([[-3.7124,  4.2021]], device='cuda:0')\n",
      "[[3.6525208e-04 9.9963474e-01]]\n",
      "[3.6525208e-04 9.9963474e-01]\n",
      "Sin 'y': Clase 1, Probabilidades [3.6525208e-04 9.9963474e-01], Cambio en probabilidad de clase original: 2.9802322387695312e-06\n",
      "tensor([[-3.7186,  4.2307]], device='cuda:0')\n",
      "[[3.5277425e-04 9.9964726e-01]]\n",
      "[3.5277425e-04 9.9964726e-01]\n",
      "Sin 'agradable': Clase 1, Probabilidades [3.5277425e-04 9.9964726e-01], Cambio en probabilidad de clase original: -9.5367431640625e-06\n",
      "tensor([[-3.7322,  4.2379]], device='cuda:0')\n",
      "[[3.4550278e-04 9.9965453e-01]]\n",
      "[3.4550278e-04 9.9965453e-01]\n",
      "Sin 'que': Clase 1, Probabilidades [3.4550278e-04 9.9965453e-01], Cambio en probabilidad de clase original: -1.6808509826660156e-05\n",
      "tensor([[-3.7114,  4.2127]], device='cuda:0')\n",
      "[[3.618059e-04 9.996382e-01]]\n",
      "[3.618059e-04 9.996382e-01]\n",
      "Sin 'otras': Clase 1, Probabilidades [3.618059e-04 9.996382e-01], Cambio en probabilidad de clase original: -4.76837158203125e-07\n",
      "tensor([[-3.7033,  4.2008]], device='cuda:0')\n",
      "[[3.6907432e-04 9.9963093e-01]]\n",
      "[3.6907432e-04 9.9963093e-01]\n",
      "Sin 'películas': Clase 1, Probabilidades [3.6907432e-04 9.9963093e-01], Cambio en probabilidad de clase original: 6.794929504394531e-06\n",
      "tensor([[-3.7213,  4.2309]], device='cuda:0')\n",
      "[[3.5177078e-04 9.9964821e-01]]\n",
      "[3.5177078e-04 9.9964821e-01]\n",
      "Sin 'que': Clase 1, Probabilidades [3.5177078e-04 9.9964821e-01], Cambio en probabilidad de clase original: -1.049041748046875e-05\n",
      "tensor([[-3.7075,  4.2064]], device='cuda:0')\n",
      "[[3.6547988e-04 9.9963450e-01]]\n",
      "[3.6547988e-04 9.9963450e-01]\n",
      "Sin 'he': Clase 1, Probabilidades [3.6547988e-04 9.9963450e-01], Cambio en probabilidad de clase original: 3.2186508178710938e-06\n",
      "tensor([[-3.7126,  4.2065]], device='cuda:0')\n",
      "[[3.6360256e-04 9.9963641e-01]]\n",
      "[3.6360256e-04 9.9963641e-01]\n",
      "Sin 'visto': Clase 1, Probabilidades [3.6360256e-04 9.9963641e-01], Cambio en probabilidad de clase original: 1.3113021850585938e-06\n",
      "tensor([[-3.7093,  4.2100]], device='cuda:0')\n",
      "[[3.635261e-04 9.996364e-01]]\n",
      "[3.635261e-04 9.996364e-01]\n",
      "Sin 'en': Clase 1, Probabilidades [3.635261e-04 9.996364e-01], Cambio en probabilidad de clase original: 1.3113021850585938e-06\n",
      "tensor([[-3.7130,  4.2168]], device='cuda:0')\n",
      "[[3.5975326e-04 9.9964023e-01]]\n",
      "[3.5975326e-04 9.9964023e-01]\n",
      "Sin 'el': Clase 1, Probabilidades [3.5975326e-04 9.9964023e-01], Cambio en probabilidad de clase original: -2.5033950805664062e-06\n",
      "tensor([[-3.7079,  4.2083]], device='cuda:0')\n",
      "[[3.6463945e-04 9.9963534e-01]]\n",
      "[3.6463945e-04 9.9963534e-01]\n",
      "Sin 'cine': Clase 1, Probabilidades [3.6463945e-04 9.9963534e-01], Cambio en probabilidad de clase original: 2.384185791015625e-06\n",
      "tensor([[-3.6908,  4.1845]], device='cuda:0')\n",
      "[[3.798726e-04 9.996201e-01]]\n",
      "[3.798726e-04 9.996201e-01]\n",
      "Sin 'últimamente': Clase 1, Probabilidades [3.798726e-04 9.996201e-01], Cambio en probabilidad de clase original: 1.7642974853515625e-05\n",
      "tensor([[-3.7090,  4.2089]], device='cuda:0')\n",
      "[[3.6402603e-04 9.9963593e-01]]\n",
      "[3.6402603e-04 9.9963593e-01]\n",
      "Sin ',': Clase 1, Probabilidades [3.6402603e-04 9.9963593e-01], Cambio en probabilidad de clase original: 1.7881393432617188e-06\n",
      "tensor([[-3.7091,  4.2074]], device='cuda:0')\n",
      "[[3.6456450e-04 9.9963546e-01]]\n",
      "[3.6456450e-04 9.9963546e-01]\n",
      "Sin 'como': Clase 1, Probabilidades [3.6456450e-04 9.9963546e-01], Cambio en probabilidad de clase original: 2.2649765014648438e-06\n",
      "tensor([[-3.7072,  4.2150]], device='cuda:0')\n",
      "[[3.624729e-04 9.996375e-01]]\n",
      "[3.624729e-04 9.996375e-01]\n",
      "Sin 'Saw': Clase 1, Probabilidades [3.624729e-04 9.996375e-01], Cambio en probabilidad de clase original: 2.384185791015625e-07\n",
      "tensor([[-3.7218,  4.2314]], device='cuda:0')\n",
      "[[3.514148e-04 9.996486e-01]]\n",
      "[3.514148e-04 9.996486e-01]\n",
      "Sin 'III': Clase 1, Probabilidades [3.514148e-04 9.996486e-01], Cambio en probabilidad de clase original: -1.0848045349121094e-05\n",
      "tensor([[-3.7135,  4.2172]], device='cuda:0')\n",
      "[[3.594139e-04 9.996406e-01]]\n",
      "[3.594139e-04 9.996406e-01]\n",
      "Sin '(': Clase 1, Probabilidades [3.594139e-04 9.996406e-01], Cambio en probabilidad de clase original: -2.86102294921875e-06\n",
      "tensor([[-3.7127,  4.2168]], device='cuda:0')\n",
      "[[3.5983452e-04 9.9964011e-01]]\n",
      "[3.5983452e-04 9.9964011e-01]\n",
      "Sin 'aburrida': Clase 1, Probabilidades [3.5983452e-04 9.9964011e-01], Cambio en probabilidad de clase original: -2.384185791015625e-06\n",
      "tensor([[-3.7110,  4.2136]], device='cuda:0')\n",
      "[[3.6161518e-04 9.9963832e-01]]\n",
      "[3.6161518e-04 9.9963832e-01]\n",
      "Sin 'y': Clase 1, Probabilidades [3.6161518e-04 9.9963832e-01], Cambio en probabilidad de clase original: -5.960464477539062e-07\n",
      "tensor([[-3.7125,  4.2157]], device='cuda:0')\n",
      "[[3.6028321e-04 9.9963975e-01]]\n",
      "[3.6028321e-04 9.9963975e-01]\n",
      "Sin 'lenta': Clase 1, Probabilidades [3.6028321e-04 9.9963975e-01], Cambio en probabilidad de clase original: -2.0265579223632812e-06\n",
      "tensor([[-3.7135,  4.2155]], device='cuda:0')\n",
      "[[3.6002637e-04 9.9963999e-01]]\n",
      "[3.6002637e-04 9.9963999e-01]\n",
      "Sin ')': Clase 1, Probabilidades [3.6002637e-04 9.9963999e-01], Cambio en probabilidad de clase original: -2.2649765014648438e-06\n",
      "tensor([[-3.7140,  4.2161]], device='cuda:0')\n",
      "[[3.5960550e-04 9.9964035e-01]]\n",
      "[3.5960550e-04 9.9964035e-01]\n",
      "Sin 'o': Clase 1, Probabilidades [3.5960550e-04 9.9964035e-01], Cambio en probabilidad de clase original: -2.6226043701171875e-06\n",
      "tensor([[-3.7106,  4.2178]], device='cuda:0')\n",
      "[[3.6024163e-04 9.9963975e-01]]\n",
      "[3.6024163e-04 9.9963975e-01]\n",
      "Sin 'Casino': Clase 1, Probabilidades [3.6024163e-04 9.9963975e-01], Cambio en probabilidad de clase original: -2.0265579223632812e-06\n",
      "tensor([[-3.7123,  4.2195]], device='cuda:0')\n",
      "[[3.5899319e-04 9.9964094e-01]]\n",
      "[3.5899319e-04 9.9964094e-01]\n",
      "Sin 'Royale': Clase 1, Probabilidades [3.5899319e-04 9.9964094e-01], Cambio en probabilidad de clase original: -3.2186508178710938e-06\n",
      "tensor([[-3.7097,  4.2120]], device='cuda:0')\n",
      "[[3.6267133e-04 9.9963737e-01]]\n",
      "[3.6267133e-04 9.9963737e-01]\n",
      "Sin '(': Clase 1, Probabilidades [3.6267133e-04 9.9963737e-01], Cambio en probabilidad de clase original: 3.5762786865234375e-07\n",
      "tensor([[-3.7100,  4.2127]], device='cuda:0')\n",
      "[[3.622824e-04 9.996377e-01]]\n",
      "[3.622824e-04 9.996377e-01]\n",
      "Sin 'demasiado': Clase 1, Probabilidades [3.622824e-04 9.996377e-01], Cambio en probabilidad de clase original: 0.0\n",
      "tensor([[-3.7135,  4.2246]], device='cuda:0')\n",
      "[[3.5676514e-04 9.9964321e-01]]\n",
      "[3.5676514e-04 9.9964321e-01]\n",
      "Sin 'homoerótica': Clase 1, Probabilidades [3.5676514e-04 9.9964321e-01], Cambio en probabilidad de clase original: -5.4836273193359375e-06\n",
      "tensor([[-3.7131,  4.2160]], device='cuda:0')\n",
      "[[3.5998123e-04 9.9963999e-01]]\n",
      "[3.5998123e-04 9.9963999e-01]\n",
      "Sin ')': Clase 1, Probabilidades [3.5998123e-04 9.9963999e-01], Cambio en probabilidad de clase original: -2.2649765014648438e-06\n",
      "tensor([[-3.7127,  4.2145]], device='cuda:0')\n",
      "[[3.6067632e-04 9.9963927e-01]]\n",
      "[3.6067632e-04 9.9963927e-01]\n",
      "Sin ',': Clase 1, Probabilidades [3.6067632e-04 9.9963927e-01], Cambio en probabilidad de clase original: -1.5497207641601562e-06\n",
      "tensor([[-3.7208,  4.2092]], device='cuda:0')\n",
      "[[3.5966892e-04 9.9964035e-01]]\n",
      "[3.5966892e-04 9.9964035e-01]\n",
      "Sin 'aunque': Clase 1, Probabilidades [3.5966892e-04 9.9964035e-01], Cambio en probabilidad de clase original: -2.6226043701171875e-06\n",
      "tensor([[-3.7136,  4.2165]], device='cuda:0')\n",
      "[[3.5961985e-04 9.9964035e-01]]\n",
      "[3.5961985e-04 9.9964035e-01]\n",
      "Sin 'he': Clase 1, Probabilidades [3.5961985e-04 9.9964035e-01], Cambio en probabilidad de clase original: -2.6226043701171875e-06\n",
      "tensor([[-3.7135,  4.2173]], device='cuda:0')\n",
      "[[3.593546e-04 9.996406e-01]]\n",
      "[3.593546e-04 9.996406e-01]\n",
      "Sin 'disfrutado': Clase 1, Probabilidades [3.593546e-04 9.996406e-01], Cambio en probabilidad de clase original: -2.86102294921875e-06\n",
      "tensor([[-3.7102,  4.2133]], device='cuda:0')\n",
      "[[3.6200046e-04 9.9963796e-01]]\n",
      "[3.6200046e-04 9.9963796e-01]\n",
      "Sin 'mucho': Clase 1, Probabilidades [3.6200046e-04 9.9963796e-01], Cambio en probabilidad de clase original: -2.384185791015625e-07\n",
      "tensor([[-3.7119,  4.2130]], device='cuda:0')\n",
      "[[3.6146372e-04 9.9963856e-01]]\n",
      "[3.6146372e-04 9.9963856e-01]\n",
      "Sin 'entregas': Clase 1, Probabilidades [3.6146372e-04 9.9963856e-01], Cambio en probabilidad de clase original: -8.344650268554688e-07\n",
      "tensor([[-3.6971,  4.1980]], device='cuda:0')\n",
      "[[3.7240933e-04 9.9962759e-01]]\n",
      "[3.7240933e-04 9.9962759e-01]\n",
      "Sin 'anteriores': Clase 1, Probabilidades [3.7240933e-04 9.9962759e-01], Cambio en probabilidad de clase original: 1.0132789611816406e-05\n",
      "tensor([[-3.7161,  4.2192]], device='cuda:0')\n",
      "[[3.5773034e-04 9.9964225e-01]]\n",
      "[3.5773034e-04 9.9964225e-01]\n",
      "Sin '.': Clase 1, Probabilidades [3.5773034e-04 9.9964225e-01], Cambio en probabilidad de clase original: -4.5299530029296875e-06\n",
      "tensor([[-3.7179,  4.2180]], device='cuda:0')\n",
      "[[3.5754163e-04 9.9964249e-01]]\n",
      "[3.5754163e-04 9.9964249e-01]\n",
      "Sin 'Si': Clase 1, Probabilidades [3.5754163e-04 9.9964249e-01], Cambio en probabilidad de clase original: -4.76837158203125e-06\n",
      "tensor([[-3.7153,  4.2225]], device='cuda:0')\n",
      "[[3.568709e-04 9.996431e-01]]\n",
      "[3.568709e-04 9.996431e-01]\n",
      "Sin 'disfrutaste': Clase 1, Probabilidades [3.568709e-04 9.996431e-01], Cambio en probabilidad de clase original: -5.364418029785156e-06\n",
      "tensor([[-3.7094,  4.2230]], device='cuda:0')\n",
      "[[3.5877028e-04 9.9964118e-01]]\n",
      "[3.5877028e-04 9.9964118e-01]\n",
      "Sin 'Borat': Clase 1, Probabilidades [3.5877028e-04 9.9964118e-01], Cambio en probabilidad de clase original: -3.4570693969726562e-06\n",
      "tensor([[-3.7146,  4.2158]], device='cuda:0')\n",
      "[[3.5953760e-04 9.9964046e-01]]\n",
      "[3.5953760e-04 9.9964046e-01]\n",
      "Sin ',': Clase 1, Probabilidades [3.5953760e-04 9.9964046e-01], Cambio en probabilidad de clase original: -2.7418136596679688e-06\n",
      "tensor([[-3.7135,  4.2174]], device='cuda:0')\n",
      "[[3.5932893e-04 9.9964070e-01]]\n",
      "[3.5932893e-04 9.9964070e-01]\n",
      "Sin 'seguramente': Clase 1, Probabilidades [3.5932893e-04 9.9964070e-01], Cambio en probabilidad de clase original: -2.9802322387695312e-06\n",
      "tensor([[-3.7071,  4.2089]], device='cuda:0')\n",
      "[[3.6472926e-04 9.9963522e-01]]\n",
      "[3.6472926e-04 9.9963522e-01]\n",
      "Sin 'disfrutarás': Clase 1, Probabilidades [3.6472926e-04 9.9963522e-01], Cambio en probabilidad de clase original: 2.5033950805664062e-06\n",
      "tensor([[-3.7135,  4.2178]], device='cuda:0')\n",
      "[[3.5919333e-04 9.9964082e-01]]\n",
      "[3.5919333e-04 9.9964082e-01]\n",
      "Sin 'la': Clase 1, Probabilidades [3.5919333e-04 9.9964082e-01], Cambio en probabilidad de clase original: -3.0994415283203125e-06\n",
      "tensor([[-3.7062,  4.2099]], device='cuda:0')\n",
      "[[3.6468392e-04 9.9963534e-01]]\n",
      "[3.6468392e-04 9.9963534e-01]\n",
      "Sin 'historia': Clase 1, Probabilidades [3.6468392e-04 9.9963534e-01], Cambio en probabilidad de clase original: 2.384185791015625e-06\n",
      "tensor([[-3.7065,  4.2119]], device='cuda:0')\n",
      "[[3.6382498e-04 9.9963617e-01]]\n",
      "[3.6382498e-04 9.9963617e-01]\n",
      "Sin 'de': Clase 1, Probabilidades [3.6382498e-04 9.9963617e-01], Cambio en probabilidad de clase original: 1.5497207641601562e-06\n",
      "tensor([[-3.7114,  4.2144]], device='cuda:0')\n",
      "[[3.6114352e-04 9.9963880e-01]]\n",
      "[3.6114352e-04 9.9963880e-01]\n",
      "Sin 'The': Clase 1, Probabilidades [3.6114352e-04 9.9963880e-01], Cambio en probabilidad de clase original: -1.0728836059570312e-06\n",
      "tensor([[-3.7100,  4.2127]], device='cuda:0')\n",
      "[[3.622822e-04 9.996377e-01]]\n",
      "[3.622822e-04 9.996377e-01]\n",
      "Sin 'Greatest': Clase 1, Probabilidades [3.622822e-04 9.996377e-01], Cambio en probabilidad de clase original: 0.0\n",
      "tensor([[-3.7110,  4.2135]], device='cuda:0')\n",
      "[[3.6164658e-04 9.9963832e-01]]\n",
      "[3.6164658e-04 9.9963832e-01]\n",
      "Sin 'Band': Clase 1, Probabilidades [3.6164658e-04 9.9963832e-01], Cambio en probabilidad de clase original: -5.960464477539062e-07\n",
      "tensor([[-3.7112,  4.2135]], device='cuda:0')\n",
      "[[3.6155659e-04 9.9963844e-01]]\n",
      "[3.6155659e-04 9.9963844e-01]\n",
      "Sin 'on': Clase 1, Probabilidades [3.6155659e-04 9.9963844e-01], Cambio en probabilidad de clase original: -7.152557373046875e-07\n",
      "tensor([[-3.7185,  4.2253]], device='cuda:0')\n",
      "[[3.5472689e-04 9.9964523e-01]]\n",
      "[3.5472689e-04 9.9964523e-01]\n",
      "Sin 'Earth': Clase 1, Probabilidades [3.5472689e-04 9.9964523e-01], Cambio en probabilidad de clase original: -7.510185241699219e-06\n",
      "tensor([[-3.7157,  4.2343]], device='cuda:0')\n",
      "[[3.5256174e-04 9.9964738e-01]]\n",
      "[3.5256174e-04 9.9964738e-01]\n",
      "Sin '.': Clase 1, Probabilidades [3.5256174e-04 9.9964738e-01], Cambio en probabilidad de clase original: -9.655952453613281e-06\n",
      "\n",
      "Palabras ordenadas por impacto en la predicción de la clase original:\n",
      "Palabra 'enganchado': Cambio en probabilidad de clase original 0.00007111, Nueva clase: 1\n",
      "Palabra 'pesar': Cambio en probabilidad de clase original 0.00004119, Nueva clase: 1\n",
      "Palabra '.': Cambio en probabilidad de clase original 0.00002182, Nueva clase: 1\n",
      "Palabra 'retratan': Cambio en probabilidad de clase original 0.00002170, Nueva clase: 1\n",
      "Palabra 'escuché': Cambio en probabilidad de clase original 0.00002146, Nueva clase: 1\n",
      "Palabra 'absurda': Cambio en probabilidad de clase original 0.00002110, Nueva clase: 1\n",
      "Palabra 'álbum': Cambio en probabilidad de clase original 0.00002015, Nueva clase: 1\n",
      "Palabra 'primer': Cambio en probabilidad de clase original 0.00001979, Nueva clase: 1\n",
      "Palabra 'trama': Cambio en probabilidad de clase original 0.00001860, Nueva clase: 1\n",
      "Palabra 'más': Cambio en probabilidad de clase original -0.00001836, Nueva clase: 1\n",
      "Palabra 'últimamente': Cambio en probabilidad de clase original 0.00001764, Nueva clase: 1\n",
      "Palabra 'Después': Cambio en probabilidad de clase original 0.00001752, Nueva clase: 1\n",
      "Palabra 'Jaybles': Cambio en probabilidad de clase original 0.00001740, Nueva clase: 1\n",
      "Palabra 'entretenida': Cambio en probabilidad de clase original -0.00001740, Nueva clase: 1\n",
      "Palabra 'yo': Cambio en probabilidad de clase original 0.00001717, Nueva clase: 1\n",
      "Palabra 'canciones': Cambio en probabilidad de clase original 0.00001717, Nueva clase: 1\n",
      "Palabra '.': Cambio en probabilidad de clase original 0.00001705, Nueva clase: 1\n",
      "Palabra 'perspectiva': Cambio en probabilidad de clase original 0.00001681, Nueva clase: 1\n",
      "Palabra 'que': Cambio en probabilidad de clase original -0.00001681, Nueva clase: 1\n",
      "Palabra 'Originalmente': Cambio en probabilidad de clase original 0.00001669, Nueva clase: 1\n",
      "Palabra 'y': Cambio en probabilidad de clase original 0.00001669, Nueva clase: 1\n",
      "Palabra 'era': Cambio en probabilidad de clase original 0.00001657, Nueva clase: 1\n",
      "Palabra 'su': Cambio en probabilidad de clase original 0.00001609, Nueva clase: 1\n",
      "Palabra 'de': Cambio en probabilidad de clase original 0.00001538, Nueva clase: 1\n",
      "Palabra 'naturalmente': Cambio en probabilidad de clase original 0.00001502, Nueva clase: 1\n",
      "Palabra ',': Cambio en probabilidad de clase original 0.00001466, Nueva clase: 1\n",
      "Palabra 'por': Cambio en probabilidad de clase original 0.00001419, Nueva clase: 1\n",
      "Palabra ',': Cambio en probabilidad de clase original 0.00001407, Nueva clase: 1\n",
      "Palabra 'algunas': Cambio en probabilidad de clase original 0.00001407, Nueva clase: 1\n",
      "Palabra ',': Cambio en probabilidad de clase original 0.00001407, Nueva clase: 1\n",
      "Palabra 'fanático': Cambio en probabilidad de clase original 0.00001383, Nueva clase: 1\n",
      "Palabra 'The': Cambio en probabilidad de clase original 0.00001359, Nueva clase: 1\n",
      "Palabra 'mi': Cambio en probabilidad de clase original 0.00001335, Nueva clase: 1\n",
      "Palabra 'de': Cambio en probabilidad de clase original 0.00001287, Nueva clase: 1\n",
      "Palabra 'de': Cambio en probabilidad de clase original 0.00001276, Nueva clase: 1\n",
      "Palabra 'KG': Cambio en probabilidad de clase original 0.00001276, Nueva clase: 1\n",
      "Palabra ',': Cambio en probabilidad de clase original 0.00001216, Nueva clase: 1\n",
      "Palabra 'D': Cambio en probabilidad de clase original 0.00001204, Nueva clase: 1\n",
      "Palabra 'P.O.D.': Cambio en probabilidad de clase original -0.00001204, Nueva clase: 1\n",
      "Palabra 'bastante': Cambio en probabilidad de clase original 0.00001204, Nueva clase: 1\n",
      "Palabra 'bastante': Cambio en probabilidad de clase original 0.00001192, Nueva clase: 1\n",
      "Palabra 'era': Cambio en probabilidad de clase original 0.00001180, Nueva clase: 1\n",
      "Palabra ',': Cambio en probabilidad de clase original 0.00001156, Nueva clase: 1\n",
      "Palabra 'pero': Cambio en probabilidad de clase original 0.00001156, Nueva clase: 1\n",
      "Palabra 'que': Cambio en probabilidad de clase original 0.00001132, Nueva clase: 1\n",
      "Palabra 'cambió': Cambio en probabilidad de clase original 0.00001121, Nueva clase: 1\n",
      "Palabra 'y': Cambio en probabilidad de clase original 0.00001121, Nueva clase: 1\n",
      "Palabra ',': Cambio en probabilidad de clase original 0.00001109, Nueva clase: 1\n",
      "Palabra 'ver': Cambio en probabilidad de clase original 0.00001097, Nueva clase: 1\n",
      "Palabra 'es': Cambio en probabilidad de clase original 0.00001097, Nueva clase: 1\n",
      "Palabra 'de': Cambio en probabilidad de clase original 0.00001097, Nueva clase: 1\n",
      "Palabra 'de': Cambio en probabilidad de clase original 0.00001085, Nueva clase: 1\n",
      "Palabra 'III': Cambio en probabilidad de clase original -0.00001085, Nueva clase: 1\n",
      "Palabra 'Tenacious': Cambio en probabilidad de clase original 0.00001073, Nueva clase: 1\n",
      "Palabra 'que': Cambio en probabilidad de clase original 0.00001073, Nueva clase: 1\n",
      "Palabra ',': Cambio en probabilidad de clase original 0.00001049, Nueva clase: 1\n",
      "Palabra 'que': Cambio en probabilidad de clase original -0.00001049, Nueva clase: 1\n",
      "Palabra 'anteriores': Cambio en probabilidad de clase original 0.00001013, Nueva clase: 1\n",
      "Palabra 'principio': Cambio en probabilidad de clase original 0.00001001, Nueva clase: 1\n",
      "Palabra 'la': Cambio en probabilidad de clase original 0.00000989, Nueva clase: 1\n",
      "Palabra '.': Cambio en probabilidad de clase original -0.00000966, Nueva clase: 1\n",
      "Palabra 'agradable': Cambio en probabilidad de clase original -0.00000954, Nueva clase: 1\n",
      "Palabra 'la': Cambio en probabilidad de clase original 0.00000930, Nueva clase: 1\n",
      "Palabra 'las': Cambio en probabilidad de clase original 0.00000858, Nueva clase: 1\n",
      "Palabra 'Mucho': Cambio en probabilidad de clase original 0.00000858, Nueva clase: 1\n",
      "Palabra 'en': Cambio en probabilidad de clase original 0.00000846, Nueva clase: 1\n",
      "Palabra 'película': Cambio en probabilidad de clase original 0.00000834, Nueva clase: 1\n",
      "Palabra 'la': Cambio en probabilidad de clase original 0.00000823, Nueva clase: 1\n",
      "Palabra 'actitudes': Cambio en probabilidad de clase original 0.00000811, Nueva clase: 1\n",
      "Palabra 'y': Cambio en probabilidad de clase original 0.00000775, Nueva clase: 1\n",
      "Palabra 'Earth': Cambio en probabilidad de clase original -0.00000751, Nueva clase: 1\n",
      "Palabra 'películas': Cambio en probabilidad de clase original 0.00000679, Nueva clase: 1\n",
      "Palabra 'a': Cambio en probabilidad de clase original 0.00000644, Nueva clase: 1\n",
      "Palabra 'gracias': Cambio en probabilidad de clase original 0.00000632, Nueva clase: 1\n",
      "Palabra 'La': Cambio en probabilidad de clase original 0.00000620, Nueva clase: 1\n",
      "Palabra 'homoerótica': Cambio en probabilidad de clase original -0.00000548, Nueva clase: 1\n",
      "Palabra 'disfrutaste': Cambio en probabilidad de clase original -0.00000536, Nueva clase: 1\n",
      "Palabra 'fin': Cambio en probabilidad de clase original 0.00000513, Nueva clase: 1\n",
      "Palabra 'quedé': Cambio en probabilidad de clase original 0.00000501, Nueva clase: 1\n",
      "Palabra 'Si': Cambio en probabilidad de clase original -0.00000477, Nueva clase: 1\n",
      "Palabra 'encontré': Cambio en probabilidad de clase original 0.00000453, Nueva clase: 1\n",
      "Palabra '.': Cambio en probabilidad de clase original -0.00000453, Nueva clase: 1\n",
      "Palabra 'película': Cambio en probabilidad de clase original 0.00000429, Nueva clase: 1\n",
      "Palabra 'me': Cambio en probabilidad de clase original -0.00000417, Nueva clase: 1\n",
      "Palabra 'a': Cambio en probabilidad de clase original 0.00000358, Nueva clase: 1\n",
      "Palabra 'Borat': Cambio en probabilidad de clase original -0.00000346, Nueva clase: 1\n",
      "Palabra 'película': Cambio en probabilidad de clase original 0.00000334, Nueva clase: 1\n",
      "Palabra 'a': Cambio en probabilidad de clase original 0.00000334, Nueva clase: 1\n",
      "Palabra 'he': Cambio en probabilidad de clase original 0.00000322, Nueva clase: 1\n",
      "Palabra 'Royale': Cambio en probabilidad de clase original -0.00000322, Nueva clase: 1\n",
      "Palabra 'la': Cambio en probabilidad de clase original -0.00000310, Nueva clase: 1\n",
      "Palabra 'y': Cambio en probabilidad de clase original 0.00000298, Nueva clase: 1\n",
      "Palabra 'seguramente': Cambio en probabilidad de clase original -0.00000298, Nueva clase: 1\n",
      "Palabra 'realmente': Cambio en probabilidad de clase original 0.00000286, Nueva clase: 1\n",
      "Palabra '(': Cambio en probabilidad de clase original -0.00000286, Nueva clase: 1\n",
      "Palabra 'disfrutado': Cambio en probabilidad de clase original -0.00000286, Nueva clase: 1\n",
      "Palabra ',': Cambio en probabilidad de clase original -0.00000274, Nueva clase: 1\n",
      "Palabra 'o': Cambio en probabilidad de clase original -0.00000262, Nueva clase: 1\n",
      "Palabra 'aunque': Cambio en probabilidad de clase original -0.00000262, Nueva clase: 1\n",
      "Palabra 'he': Cambio en probabilidad de clase original -0.00000262, Nueva clase: 1\n",
      "Palabra 'el': Cambio en probabilidad de clase original -0.00000250, Nueva clase: 1\n",
      "Palabra 'disfrutarás': Cambio en probabilidad de clase original 0.00000250, Nueva clase: 1\n",
      "Palabra 'cine': Cambio en probabilidad de clase original 0.00000238, Nueva clase: 1\n",
      "Palabra 'aburrida': Cambio en probabilidad de clase original -0.00000238, Nueva clase: 1\n",
      "Palabra 'historia': Cambio en probabilidad de clase original 0.00000238, Nueva clase: 1\n",
      "Palabra 'como': Cambio en probabilidad de clase original 0.00000226, Nueva clase: 1\n",
      "Palabra ')': Cambio en probabilidad de clase original -0.00000226, Nueva clase: 1\n",
      "Palabra ')': Cambio en probabilidad de clase original -0.00000226, Nueva clase: 1\n",
      "Palabra 'lenta': Cambio en probabilidad de clase original -0.00000203, Nueva clase: 1\n",
      "Palabra 'Casino': Cambio en probabilidad de clase original -0.00000203, Nueva clase: 1\n",
      "Palabra ',': Cambio en probabilidad de clase original 0.00000179, Nueva clase: 1\n",
      "Palabra ',': Cambio en probabilidad de clase original -0.00000155, Nueva clase: 1\n",
      "Palabra 'de': Cambio en probabilidad de clase original 0.00000155, Nueva clase: 1\n",
      "Palabra 'divertida': Cambio en probabilidad de clase original -0.00000131, Nueva clase: 1\n",
      "Palabra 'visto': Cambio en probabilidad de clase original 0.00000131, Nueva clase: 1\n",
      "Palabra 'en': Cambio en probabilidad de clase original 0.00000131, Nueva clase: 1\n",
      "Palabra 'decepcionado': Cambio en probabilidad de clase original -0.00000119, Nueva clase: 1\n",
      "Palabra '.': Cambio en probabilidad de clase original 0.00000119, Nueva clase: 1\n",
      "Palabra 'The': Cambio en probabilidad de clase original -0.00000107, Nueva clase: 1\n",
      "Palabra 'entregas': Cambio en probabilidad de clase original -0.00000083, Nueva clase: 1\n",
      "Palabra 'on': Cambio en probabilidad de clase original -0.00000072, Nueva clase: 1\n",
      "Palabra 'y': Cambio en probabilidad de clase original -0.00000060, Nueva clase: 1\n",
      "Palabra 'Band': Cambio en probabilidad de clase original -0.00000060, Nueva clase: 1\n",
      "Palabra 'otras': Cambio en probabilidad de clase original -0.00000048, Nueva clase: 1\n",
      "Palabra '(': Cambio en probabilidad de clase original 0.00000036, Nueva clase: 1\n",
      "Palabra 'Saw': Cambio en probabilidad de clase original 0.00000024, Nueva clase: 1\n",
      "Palabra 'mucho': Cambio en probabilidad de clase original -0.00000024, Nueva clase: 1\n",
      "Palabra 'demasiado': Cambio en probabilidad de clase original 0.00000000, Nueva clase: 1\n",
      "Palabra 'Greatest': Cambio en probabilidad de clase original 0.00000000, Nueva clase: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "\n",
    "# Cargar el modelo en español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "frase = \"¡Hola! Esto es un ejemplo con spaCy.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Cargar el modelo y el tokenizador preentrenados\n",
    "output_dir = nombredecarpetaatrabajar + \"/stopwords/\" + \"/model\"\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "loaded_model = BertForSequenceClassification.from_pretrained(output_dir)\n",
    "\n",
    "# Verificar si CUDA está disponible y mover el modelo a la GPU si es posible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()  # Colocar en modo evaluación\n",
    "\n",
    "# Función para obtener la predicción de una oración\n",
    "def predict_sentiment(text):\n",
    "    inputs = loaded_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Mover los inputs a la GPU\n",
    "    with torch.no_grad():  # Desactivar el cálculo de gradientes\n",
    "        outputs = loaded_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    print(logits)\n",
    "    probabilities = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "    print(probabilities)\n",
    "     \n",
    "    return probabilities[0]\n",
    "\n",
    "# Función para analizar el impacto de cada palabra en la clasificación\n",
    "def analyze_word_importance(text):\n",
    "    # Obtener la predicción de la oración original\n",
    "    original_probs = predict_sentiment(text)\n",
    "    original_class = np.argmax(original_probs)\n",
    "    print(f\"Texto original: '{text}'\")\n",
    "    print(f\"Predicción original: Clase {original_class}, Probabilidades {original_probs}\")\n",
    "\n",
    "    # Tokenizar el texto y convertirlo en tokens individuales\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    word_importance = []\n",
    "\n",
    "    # Perturbar el texto eliminando una palabra a la vez\n",
    "    for i, token in enumerate(tokens):\n",
    "        # Crear un texto nuevo sin el token actual\n",
    "        perturbed_tokens = tokens[:i] + tokens[i+1:]\n",
    "        perturbed_text = loaded_tokenizer.convert_tokens_to_string(perturbed_tokens)\n",
    "\n",
    "        # Obtener la predicción del texto perturbado\n",
    "        perturbed_probs = predict_sentiment(perturbed_text)\n",
    "        print(perturbed_probs)\n",
    "        perturbed_class = np.argmax(perturbed_probs)\n",
    "\n",
    "        # Calcular el cambio en la probabilidad para la clase original\n",
    "        prob_change = original_probs[original_class] - perturbed_probs[original_class]\n",
    "        \n",
    "        # Guardar el resultado\n",
    "        word_importance.append((token, prob_change, perturbed_class))\n",
    "        print(f\"Sin '{token}': Clase {perturbed_class}, Probabilidades {perturbed_probs}, Cambio en probabilidad de clase original: {prob_change}\")\n",
    "\n",
    "    # Ordenar las palabras por importancia en la probabilidad de la clase original\n",
    "    word_importance = sorted(word_importance, key=lambda x: abs(x[1]), reverse=True)\n",
    "    print(\"\\nPalabras ordenadas por impacto en la predicción de la clase original:\")\n",
    "    for token, change, new_class in word_importance:\n",
    "        print(f\"Palabra '{token}': Cambio en probabilidad de clase original {change:.8f}, Nueva clase: {new_class}\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "text_example = \"Originalmente, yo era fanático de Tenacious D por su primer álbum y, naturalmente, escuché algunas canciones de The P.O.D., pero quedé bastante decepcionado. Después de ver la película, mi perspectiva cambió. La película es bastante divertida de principio a fin, y me encontré enganchado a pesar de que la trama era realmente absurda, gracias a las actitudes que KG y Jaybles retratan en la película. Mucho más entretenida y agradable que otras películas que he visto en el cine últimamente, como Saw III (aburrida y lenta) o Casino Royale (demasiado homoerótica), aunque he disfrutado mucho entregas anteriores. Si disfrutaste Borat, seguramente disfrutarás la historia de The Greatest Band on Earth.\"\n",
    "analyze_word_importance(text_example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
