{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "contador_llamadas=0\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def mostrar_arboles(text):\n",
    "    global contador_llamadas\n",
    "    \"\"\"\n",
    "    Muestra el √°rbol sint√°ctico de la frase original y luego el de la frase\n",
    "    sin preposiciones, art√≠culos, conjunciones y pronombres.\n",
    "\n",
    "    Par√°metros:\n",
    "    - text (str): La frase de entrada.\n",
    "\n",
    "    Retorno:\n",
    "    - str: La frase sin preposiciones, art√≠culos, conjunciones y pronombres.\n",
    "    \"\"\"\n",
    "    # Analizar la frase original\n",
    "    doc = nlp(text)\n",
    "\n",
    "\n",
    "    # Filtrar palabras que no son preposiciones, art√≠culos, conjunciones ni pronombres\n",
    "    filtered_words = [\n",
    "        token for token in doc\n",
    "        if token.pos_ not in {\"ADP\", \"DET\", \"CCONJ\", \"SCONJ\", \"PRON\"}\n",
    "    ]\n",
    "    # Reconstruir la frase sin estas palabras\n",
    "    filtered_sentence = \" \".join([token.text for token in filtered_words])\n",
    "    # Crear un nuevo Doc a partir de las palabras filtradas\n",
    "    filtered_sentence = re.sub(r'\\S*@\\S*\\s?', '', filtered_sentence)\n",
    "    contador_llamadas += 1\n",
    "    print(contador_llamadas)\n",
    "\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Nombre de carpeta y archivo\n",
    "nombredecarpetaatrabajar = \"REST_MEX/\"\n",
    "nombredearchivo = \"Rest_Mex_Sentiment_Analysis_2023_Train.xlsx\"\n",
    "nombre_archivo = 'CORPUS/' + nombredecarpetaatrabajar + nombredearchivo\n",
    "\n",
    "# Cargar datos desde el archivo CSV\n",
    "full_data = pd.read_excel(nombre_archivo)  # Usamos read_csv porque es un archivo CSV\n",
    "print(full_data)\n",
    "# Convertir todos los valores de 'content' a string\n",
    "full_data['lemma'] = full_data['TituloyResena'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Contar filas antes de eliminar NaN\n",
    "antes = len(full_data)\n",
    "\n",
    "# Reemplazar los valores en la columna 'Polarity'\n",
    "full_data['Polarity'] = full_data['Polarity'].replace({1: 0, 2: 0, 3 : np.nan, 4: 1, 5: 1})\n",
    "\n",
    "# Eliminar filas con NaN en la columna 'Polarity'\n",
    "full_data.dropna(subset=['Polarity'], inplace=True)\n",
    "\n",
    "print(full_data)\n",
    "\n",
    "# Contar filas despu√©s de eliminar NaN\n",
    "despues = len(full_data)\n",
    "\n",
    "print(\"N√∫mero de filas antes de eliminar NaN:\", antes)\n",
    "print(\"N√∫mero de filas despu√©s de eliminar NaN:\", despues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.to_pickle('CORPUS/'+nombredecarpetaatrabajar+'/Dataset.pkl')\n",
    "full_data = pd.read_pickle('CORPUS/'+nombredecarpetaatrabajar+'/Dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponible: True\n",
      "Nombre del dispositivo CUDA: NVIDIA GeForce RTX 2060 SUPER\n",
      "Versi√≥n de CUDA: 11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Nombre del dispositivo CUDA:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Versi√≥n de CUDA:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor en dispositivo: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "try:\n",
    "    tensor = torch.tensor([1.0, 2.0, 3.0]).to(device)\n",
    "    print(\"Tensor en dispositivo:\", tensor.device)\n",
    "except Exception as e:\n",
    "    print(\"Error al mover tensor a la GPU:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS/IMDB/IMDBDatasetSPANISH.csv\n",
      "       Unnamed: 0                                          review_en  \\\n",
      "0               0  One of the other reviewers has mentioned that ...   \n",
      "1               1  A wonderful little production. The filming tec...   \n",
      "2               2  I thought this was a wonderful way to spend ti...   \n",
      "3               3  Basically there's a family where a little boy ...   \n",
      "4               4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
      "...           ...                                                ...   \n",
      "49995       49995  I thought this movie did a down right good job...   \n",
      "49996       49996  Bad plot, bad dialogue, bad acting, idiotic di...   \n",
      "49997       49997  I am a Catholic taught in parochial elementary...   \n",
      "49998       49998  I'm going to have to disagree with the previou...   \n",
      "49999       49999  No one expects the Star Trek movies to be high...   \n",
      "\n",
      "                                               review_es sentiment  \\\n",
      "0      Uno de los otros cr√≠ticos ha mencionado que de...  positive   \n",
      "1      Una peque√±a peque√±a producci√≥n.La t√©cnica de f...  positive   \n",
      "2      Pens√© que esta era una manera maravillosa de p...  positive   \n",
      "3      B√°sicamente, hay una familia donde un ni√±o peq...  negative   \n",
      "4      El \"amor en el tiempo\" de Petter Mattei es una...  positive   \n",
      "...                                                  ...       ...   \n",
      "49995  Pens√© que esta pel√≠cula hizo un buen trabajo a...  positive   \n",
      "49996  Mala parcela, mal di√°logo, mala actuaci√≥n, dir...  negative   \n",
      "49997  Soy cat√≥lica ense√±ada en escuelas primarias pa...  negative   \n",
      "49998  Voy a tener que estar en desacuerdo con el com...  negative   \n",
      "49999  Nadie espera que las pel√≠culas de Star Trek se...  negative   \n",
      "\n",
      "       sentimiento                                              lemma  \n",
      "0                1  Uno de los otros cr√≠ticos ha mencionado que de...  \n",
      "1                1  Una peque√±a peque√±a producci√≥n.La t√©cnica de f...  \n",
      "2                1  Pens√© que esta era una manera maravillosa de p...  \n",
      "3                0  B√°sicamente, hay una familia donde un ni√±o peq...  \n",
      "4                1  El \"amor en el tiempo\" de Petter Mattei es una...  \n",
      "...            ...                                                ...  \n",
      "49995            1  Pens√© que esta pel√≠cula hizo un buen trabajo a...  \n",
      "49996            0  Mala parcela, mal di√°logo, mala actuaci√≥n, dir...  \n",
      "49997            0  Soy cat√≥lica ense√±ada en escuelas primarias pa...  \n",
      "49998            0  Voy a tener que estar en desacuerdo con el com...  \n",
      "49999            0  Nadie espera que las pel√≠culas de Star Trek se...  \n",
      "\n",
      "[50000 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GpJonat\\AppData\\Local\\Temp\\ipykernel_17616\\567952050.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['sentimiento'] = data['sentimiento'].astype(int)  # Asegurarse de que las etiquetas sean enteras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas √∫nicas: [1 0]\n",
      "N√∫mero de etiquetas √∫nicas: 2\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4061206de18549ba8299a147b7d5c7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb80694ecf14423b622ef3e3adb9d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GpJonat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8953c9135dd44d959021b1a7f26df959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4133, 'grad_norm': 4.505579471588135, 'learning_rate': 1.96024e-05, 'epoch': 0.1}\n",
      "{'loss': 0.3591, 'grad_norm': 40.17921447753906, 'learning_rate': 1.92024e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3274, 'grad_norm': 12.983050346374512, 'learning_rate': 1.88024e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3371, 'grad_norm': 0.5508607625961304, 'learning_rate': 1.8402400000000002e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3433, 'grad_norm': 9.806771278381348, 'learning_rate': 1.8002400000000002e-05, 'epoch': 0.5}\n",
      "{'loss': 0.319, 'grad_norm': 29.978900909423828, 'learning_rate': 1.76032e-05, 'epoch': 0.6}\n",
      "{'loss': 0.3006, 'grad_norm': 17.30762481689453, 'learning_rate': 1.72032e-05, 'epoch': 0.7}\n",
      "{'loss': 0.2975, 'grad_norm': 18.6928768157959, 'learning_rate': 1.6803200000000002e-05, 'epoch': 0.8}\n",
      "{'loss': 0.3139, 'grad_norm': 7.573062419891357, 'learning_rate': 1.6403200000000002e-05, 'epoch': 0.9}\n",
      "{'loss': 0.3053, 'grad_norm': 11.35680103302002, 'learning_rate': 1.60032e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d06a22a27df465aa6b87c60ca723647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36566928029060364, 'eval_runtime': 36.3426, 'eval_samples_per_second': 275.159, 'eval_steps_per_second': 34.395, 'epoch': 1.0}\n",
      "{'loss': 0.211, 'grad_norm': 0.12803401052951813, 'learning_rate': 1.5604000000000002e-05, 'epoch': 1.1}\n",
      "{'loss': 0.2074, 'grad_norm': 2.165412187576294, 'learning_rate': 1.5204e-05, 'epoch': 1.2}\n",
      "{'loss': 0.2105, 'grad_norm': 0.5245097875595093, 'learning_rate': 1.4804000000000001e-05, 'epoch': 1.3}\n",
      "{'loss': 0.2307, 'grad_norm': 0.06689663231372833, 'learning_rate': 1.4404800000000001e-05, 'epoch': 1.4}\n",
      "{'loss': 0.2237, 'grad_norm': 0.2613588273525238, 'learning_rate': 1.40048e-05, 'epoch': 1.5}\n",
      "{'loss': 0.2302, 'grad_norm': 5.528226852416992, 'learning_rate': 1.36048e-05, 'epoch': 1.6}\n",
      "{'loss': 0.1981, 'grad_norm': 14.446507453918457, 'learning_rate': 1.3204800000000003e-05, 'epoch': 1.7}\n",
      "{'loss': 0.2337, 'grad_norm': 8.71195125579834, 'learning_rate': 1.2804800000000001e-05, 'epoch': 1.8}\n",
      "{'loss': 0.2333, 'grad_norm': 17.01825714111328, 'learning_rate': 1.2404800000000002e-05, 'epoch': 1.9}\n",
      "{'loss': 0.2252, 'grad_norm': 0.08580455183982849, 'learning_rate': 1.20048e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7023f235ec43b3be11ca4b1d74d041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3394699692726135, 'eval_runtime': 36.8794, 'eval_samples_per_second': 271.154, 'eval_steps_per_second': 33.894, 'epoch': 2.0}\n",
      "{'loss': 0.117, 'grad_norm': 0.06148986890912056, 'learning_rate': 1.1605600000000001e-05, 'epoch': 2.1}\n",
      "{'loss': 0.1174, 'grad_norm': 1.4835909605026245, 'learning_rate': 1.1205600000000002e-05, 'epoch': 2.2}\n",
      "{'loss': 0.1204, 'grad_norm': 0.03757534548640251, 'learning_rate': 1.08056e-05, 'epoch': 2.3}\n",
      "{'loss': 0.1349, 'grad_norm': 0.05572096258401871, 'learning_rate': 1.04056e-05, 'epoch': 2.4}\n",
      "{'loss': 0.1362, 'grad_norm': 28.937820434570312, 'learning_rate': 1.00056e-05, 'epoch': 2.5}\n",
      "{'loss': 0.1112, 'grad_norm': 0.020086931064724922, 'learning_rate': 9.606400000000002e-06, 'epoch': 2.6}\n",
      "{'loss': 0.132, 'grad_norm': 0.034443192183971405, 'learning_rate': 9.2064e-06, 'epoch': 2.7}\n",
      "{'loss': 0.1278, 'grad_norm': 0.03326597064733505, 'learning_rate': 8.807200000000001e-06, 'epoch': 2.8}\n",
      "{'loss': 0.1357, 'grad_norm': 0.21093279123306274, 'learning_rate': 8.407200000000001e-06, 'epoch': 2.9}\n",
      "{'loss': 0.1054, 'grad_norm': 0.06044214591383934, 'learning_rate': 8.0072e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a9c171e9f94637a893b83b901adaa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.49504658579826355, 'eval_runtime': 36.2395, 'eval_samples_per_second': 275.942, 'eval_steps_per_second': 34.493, 'epoch': 3.0}\n",
      "{'loss': 0.0549, 'grad_norm': 0.009915894828736782, 'learning_rate': 7.6072e-06, 'epoch': 3.1}\n",
      "{'loss': 0.0444, 'grad_norm': 0.0093997186049819, 'learning_rate': 7.207200000000001e-06, 'epoch': 3.2}\n",
      "{'loss': 0.0645, 'grad_norm': 0.8453999161720276, 'learning_rate': 6.807200000000001e-06, 'epoch': 3.3}\n",
      "{'loss': 0.0457, 'grad_norm': 0.006219496950507164, 'learning_rate': 6.4072e-06, 'epoch': 3.4}\n",
      "{'loss': 0.0539, 'grad_norm': 34.527679443359375, 'learning_rate': 6.007200000000001e-06, 'epoch': 3.5}\n",
      "{'loss': 0.0586, 'grad_norm': 0.010505067184567451, 'learning_rate': 5.608e-06, 'epoch': 3.6}\n",
      "{'loss': 0.0416, 'grad_norm': 0.12293388694524765, 'learning_rate': 5.208000000000001e-06, 'epoch': 3.7}\n",
      "{'loss': 0.0647, 'grad_norm': 0.0144659960642457, 'learning_rate': 4.8088000000000004e-06, 'epoch': 3.8}\n",
      "{'loss': 0.0506, 'grad_norm': 0.014976494945585728, 'learning_rate': 4.4088e-06, 'epoch': 3.9}\n",
      "{'loss': 0.0591, 'grad_norm': 0.009728442877531052, 'learning_rate': 4.0088000000000005e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5dfd64a82224bcfbbf06acb11cb8a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5410951375961304, 'eval_runtime': 34.1146, 'eval_samples_per_second': 293.13, 'eval_steps_per_second': 36.641, 'epoch': 4.0}\n",
      "{'loss': 0.0223, 'grad_norm': 0.12309235334396362, 'learning_rate': 3.6088e-06, 'epoch': 4.1}\n",
      "{'loss': 0.0179, 'grad_norm': 0.0036281610373407602, 'learning_rate': 3.2096000000000006e-06, 'epoch': 4.2}\n",
      "{'loss': 0.017, 'grad_norm': 0.004299131687730551, 'learning_rate': 2.8096e-06, 'epoch': 4.3}\n",
      "{'loss': 0.0284, 'grad_norm': 0.004546491429209709, 'learning_rate': 2.4096e-06, 'epoch': 4.4}\n",
      "{'loss': 0.0249, 'grad_norm': 0.07431141287088394, 'learning_rate': 2.0096e-06, 'epoch': 4.5}\n",
      "{'loss': 0.0235, 'grad_norm': 0.00965035054832697, 'learning_rate': 1.6096e-06, 'epoch': 4.6}\n",
      "{'loss': 0.0298, 'grad_norm': 0.004322187975049019, 'learning_rate': 1.2096e-06, 'epoch': 4.7}\n",
      "{'loss': 0.0158, 'grad_norm': 0.0035967326257377863, 'learning_rate': 8.096000000000002e-07, 'epoch': 4.8}\n",
      "{'loss': 0.0193, 'grad_norm': 0.0032111613545566797, 'learning_rate': 4.0960000000000007e-07, 'epoch': 4.9}\n",
      "{'loss': 0.0262, 'grad_norm': 0.09526637941598892, 'learning_rate': 1.04e-08, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c87096ca954c618576902f529935da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.632770299911499, 'eval_runtime': 34.001, 'eval_samples_per_second': 294.109, 'eval_steps_per_second': 36.764, 'epoch': 5.0}\n",
      "{'train_runtime': 3256.703, 'train_samples_per_second': 61.412, 'train_steps_per_second': 7.676, 'train_loss': 0.15042834924697876, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eba7602d05f4dbca31cc2f9ac863ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91      4961\n",
      "           1       0.92      0.91      0.92      5039\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.91      0.92      0.91     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n",
      "Guardando el modelo y el tokenizador en: IMDB//constopwords//model\n",
      "Modelo y tokenizador guardados exitosamente.\n",
      "Cargando el modelo y el tokenizador desde: IMDB//constopwords//model\n",
      "Texto: Este producto es excelente! -> Clase Predicha: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Definir nombres de archivo\n",
    "nombredecarpetaatrabajar = \"REST_MEX/\"\n",
    "nombredearchivo = \"Rest_Mex_Sentiment_Analysis_2023_Train\"\n",
    "nombre_archivo = 'CORPUS/' + nombredecarpetaatrabajar + nombredearchivo\n",
    "print(nombre_archivo)\n",
    "\n",
    "# Cargar el DataFrame\n",
    "full_data =  pd.read_pickle('CORPUS/'+nombredecarpetaatrabajar+'/Dataset.pkl')\n",
    "print(full_data)\n",
    "\n",
    "# Seleccionar las columnas necesarias\n",
    "data = full_data[['lemma', 'sentimiento']]\n",
    "data['sentimiento'] = data['sentimiento'].astype(int)  # Asegurarse de que las etiquetas sean enteras\n",
    "\n",
    "unique_labels = data['sentimiento'].unique()\n",
    "print(\"Etiquetas √∫nicas:\", unique_labels)\n",
    "print(\"N√∫mero de etiquetas √∫nicas:\", len(unique_labels))\n",
    "\n",
    "# Dividir el DataFrame en conjunto de entrenamiento (80%) y prueba (20%)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verificar si CUDA est√° disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Cargar el tokenizador y el modelo BETO\n",
    "model_name = \"dccuchile/bert-base-spanish-wwm-cased\"  # BETO\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Cambia num_labels seg√∫n tus clases\n",
    "\n",
    "# Mover el modelo a la GPU si est√° disponible\n",
    "model.to(device)\n",
    "\n",
    "# Convertir los DataFrames a conjuntos de full_data de Hugging Face\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "# Tokenizaci√≥n\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(examples['lemma'], truncation=True, padding='max_length', max_length=256)\n",
    "    tokenized['labels'] = examples['sentimiento']  # Agregar etiquetas\n",
    "    return tokenized\n",
    "\n",
    "# Aplicar tokenizaci√≥n\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Configuraci√≥n de los argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,  # Reducido para ahorrar memoria\n",
    "    per_device_eval_batch_size=8,    # Reducido para ahorrar memoria\n",
    "    num_train_epochs=5,               # Ajustar seg√∫n necesidad\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,                        # Habilitar FP16 para ahorro de memoria\n",
    ")\n",
    "\n",
    "# Definir el entrenador\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "predictions = trainer.predict(tokenized_test)\n",
    "predicted_classes = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Convertir las etiquetas verdaderas a un formato numpy\n",
    "y_test_np = np.array(test_data['sentimiento'])\n",
    "\n",
    "# Generar el informe de clasificaci√≥n\n",
    "report = classification_report(y_test_np, predicted_classes, zero_division=0)\n",
    "print(report)\n",
    "\n",
    "# Guardar el modelo\n",
    "output_dir = nombredecarpetaatrabajar+\"/constopwords/\"+\"/model\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Crear el directorio si no existe\n",
    "print(\"Guardando el modelo y el tokenizador en:\", output_dir)\n",
    "model.save_pretrained(output_dir, safe_serialization=False)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"Modelo y tokenizador guardados exitosamente.\")\n",
    "\n",
    "# Cargar el modelo y el tokenizador para verificar\n",
    "print(\"Cargando el modelo y el tokenizador desde:\", output_dir)\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "loaded_model = BertForSequenceClassification.from_pretrained(output_dir)\n",
    "\n",
    "# Verificar si CUDA est√° disponible y mover el modelo a la GPU si es posible\n",
    "loaded_model.to(device)\n",
    "\n",
    "# Funci√≥n para predecir el sentimiento de un texto\n",
    "def predict_sentiment(text):\n",
    "    inputs = loaded_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Mover los inputs a la GPU\n",
    "    with torch.no_grad():  # Desactivar el c√°lculo de gradientes\n",
    "        outputs = loaded_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "    return predicted_class\n",
    "\n",
    "# Probar con un ejemplo\n",
    "text_example = \"Este producto es excelente!\"\n",
    "predicted_class = predict_sentiment(text_example)\n",
    "print(f\"Texto: {text_example} -> Clase Predicha: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generar la matriz de confusi√≥n\n",
    "conf_matrix = confusion_matrix(y_test_np, predicted_classes)\n",
    "\n",
    "# Etiquetas personalizadas\n",
    "labels_x = [\"Falsos Positivos (FV)\", \"Verdaderos Positivos (VV)\"]\n",
    "labels_y = [\"Falsos Negativos (VF)\", \"Verdaderos Negativos (FF)\"]\n",
    "\n",
    "# Visualizar la matriz de confusi√≥n\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels_x, yticklabels=labels_y)\n",
    "plt.title('Matriz de Confusi√≥n con Etiquetas Descriptivas')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Valores Reales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# Cargar el modelo y el tokenizador para verificar\n",
    "nombredecarpetaatrabajar = \"IMDB/\"\n",
    "nombredearchivo = \"IMDBDatasetSPANISH.csv\"\n",
    "nombre_archivo = 'CORPUS/' + nombredecarpetaatrabajar + nombredearchivo\n",
    "output_dir = nombredecarpetaatrabajar + \"/constopwords/\" + \"/model\"\n",
    "print(\"Cargando el modelo y el tokenizador desde:\", output_dir)\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "loaded_model = BertForSequenceClassification.from_pretrained(output_dir)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Verificar si CUDA est√° disponible y mover el modelo a la GPU si es posible\n",
    "loaded_model.to(device)\n",
    "\n",
    "# Funci√≥n para predecir el sentimiento de un texto\n",
    "def predict_sentiment(text):\n",
    "    inputs = loaded_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Mover los inputs a la GPU\n",
    "    with torch.no_grad():  # Desactivar el c√°lculo de gradientes\n",
    "        outputs = loaded_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "    return predicted_class\n",
    "\n",
    "# Modificar la funci√≥n para devolver probabilidades para LIME\n",
    "def predict_proba(texts):\n",
    "    inputs = loaded_tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Mover los inputs a la GPU\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=-1).cpu().numpy()  # Convertir a numpy para LIME\n",
    "    return probabilities\n",
    "\n",
    "# Probar con un ejemplo\n",
    "text_example = \"Es fant√°stico cuando una pel√≠cula comica no te hace reir.¬°¬°Qu√© pena!!Esta pel√≠cula es muy aburrida y larga.Es simplemente doloroso.La historia es da pena y no es divertido. Te sientes mejor cuando est√° termina\"\n",
    "predicted_class = predict_sentiment(text_example)\n",
    "print(f\"Texto: {text_example} -> Clase Predicha: {predicted_class}\")\n",
    "\n",
    "# Generar explicaci√≥n con LIME\n",
    "class_names = [\"Negativo\", \"Positivo\"]  # Cambiar seg√∫n las clases de tu modelo\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "# Crear explicaci√≥n para un texto\n",
    "exp = explainer.explain_instance(\n",
    "    text_example,                # Texto de ejemplo\n",
    "    predict_proba,               # Funci√≥n de predicci√≥n\n",
    "    num_features=10,             # N√∫mero de palabras m√°s importantes a mostrar\n",
    ")\n",
    "\n",
    "# Mostrar resultados en un notebook\n",
    "exp.show_in_notebook()\n",
    "\n",
    "# Guardar la explicaci√≥n en un archivo HTML\n",
    "exp.save_to_file('lime_explanation.html')\n",
    "\n",
    "# Mostrar explicaci√≥n en consola\n",
    "for label in [0, 1]:\n",
    "    print(f\"\\nEtiqueta {class_names[label]}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10780"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase predicha: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# Configurar el dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()  # Liberar memoria\n",
    "\n",
    "# Cargar el modelo y el tokenizador\n",
    "nombredecarpetaatrabajar = \"IMDB/\"\n",
    "output_dir = nombredecarpetaatrabajar + \"/stopwords/\" + \"/model\"\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "loaded_model = BertForSequenceClassification.from_pretrained(output_dir).to(device)\n",
    "\n",
    "# Funci√≥n para predecir el sentimiento\n",
    "def predict_sentiment(text):\n",
    "    inputs = loaded_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)  # Reduce max_length\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    return torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "# Funci√≥n para devolver probabilidades para LIME\n",
    "# Funci√≥n para devolver probabilidades corregida\n",
    "def predict_proba(texts):\n",
    "    batch_size = 8  # Tama√±o peque√±o para evitar saturar la GPU\n",
    "    probabilities = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = loaded_tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = loaded_model(**inputs)\n",
    "        batch_probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        probabilities.extend(batch_probs)\n",
    "    return np.array(probabilities)  # Convertimos a un numpy array\n",
    "\n",
    "# Ejemplo de texto\n",
    "text_example =\"Originalmente, yo era fan√°tico de Tenacious D por su primer √°lbum y, naturalmente, escuch√© algunas canciones de The P.O.D., pero qued√© bastante decepcionado. Despu√©s de ver la pel√≠cula, mi perspectiva cambi√≥. La pel√≠cula es bastante divertida de principio a fin, y me encontr√© enganchado a pesar de que la trama era realmente absurda, gracias a las actitudes que KG y Jaybles retratan en la pel√≠cula. Mucho m√°s entretenida y agradable que otras pel√≠culas que he visto en el cine √∫ltimamente, como Saw III (aburrida y lenta) o Casino Royale (demasiado homoer√≥tica), aunque he disfrutado mucho entregas anteriores. Si disfrutaste Borat, seguramente disfrutar√°s la historia de The Greatest Band on Earth.\"\n",
    "predicted_class = predict_sentiment(text_example)\n",
    "print(f\"Clase predicha: {predicted_class}\")\n",
    "\n",
    "# Explicaci√≥n con LIME\n",
    "class_names = [\"Negativo\", \"Positivo\"]\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "# Generar la explicaci√≥n\n",
    "exp = explainer.explain_instance(text_example, predict_proba, num_features=10)\n",
    "exp.save_to_file(\"lime_explanationlmm.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.7104,  4.2124]], device='cuda:0')\n",
      "[[3.622639e-04 9.996377e-01]]\n",
      "Texto original: 'Originalmente, yo era fan√°tico de Tenacious D por su primer √°lbum y, naturalmente, escuch√© algunas canciones de The P.O.D., pero qued√© bastante decepcionado. Despu√©s de ver la pel√≠cula, mi perspectiva cambi√≥. La pel√≠cula es bastante divertida de principio a fin, y me encontr√© enganchado a pesar de que la trama era realmente absurda, gracias a las actitudes que KG y Jaybles retratan en la pel√≠cula. Mucho m√°s entretenida y agradable que otras pel√≠culas que he visto en el cine √∫ltimamente, como Saw III (aburrida y lenta) o Casino Royale (demasiado homoer√≥tica), aunque he disfrutado mucho entregas anteriores. Si disfrutaste Borat, seguramente disfrutar√°s la historia de The Greatest Band on Earth.'\n",
      "Predicci√≥n original: Clase 1, Probabilidades [3.622639e-04 9.996377e-01]\n",
      "tensor([[-3.6969,  4.1810]], device='cuda:0')\n",
      "[[3.7889535e-04 9.9962103e-01]]\n",
      "[3.7889535e-04 9.9962103e-01]\n",
      "Sin 'Originalmente': Clase 1, Probabilidades [3.7889535e-04 9.9962103e-01], Cambio en probabilidad de clase original: 1.6689300537109375e-05\n",
      "tensor([[-3.7009,  4.1906]], device='cuda:0')\n",
      "[[3.7377080e-04 9.9962616e-01]]\n",
      "[3.7377080e-04 9.9962616e-01]\n",
      "Sin ',': Clase 1, Probabilidades [3.7377080e-04 9.9962616e-01], Cambio en probabilidad de clase original: 1.1563301086425781e-05\n",
      "tensor([[-3.6953,  4.1812]], device='cuda:0')\n",
      "[[3.7942419e-04 9.9962056e-01]]\n",
      "[3.7942419e-04 9.9962056e-01]\n",
      "Sin 'yo': Clase 1, Probabilidades [3.7942419e-04 9.9962056e-01], Cambio en probabilidad de clase original: 1.71661376953125e-05\n",
      "tensor([[-3.6956,  4.1826]], device='cuda:0')\n",
      "[[3.7877291e-04 9.9962115e-01]]\n",
      "[3.7877291e-04 9.9962115e-01]\n",
      "Sin 'era': Clase 1, Probabilidades [3.7877291e-04 9.9962115e-01], Cambio en probabilidad de clase original: 1.6570091247558594e-05\n",
      "tensor([[-3.6971,  4.1884]], device='cuda:0')\n",
      "[[3.760450e-04 9.996239e-01]]\n",
      "[3.760450e-04 9.996239e-01]\n",
      "Sin 'fan√°tico': Clase 1, Probabilidades [3.760450e-04 9.996239e-01], Cambio en probabilidad de clase original: 1.3828277587890625e-05\n",
      "tensor([[-3.6973,  4.1838]], device='cuda:0')\n",
      "[[3.7767863e-04 9.9962234e-01]]\n",
      "[3.7767863e-04 9.9962234e-01]\n",
      "Sin 'de': Clase 1, Probabilidades [3.7767863e-04 9.9962234e-01], Cambio en probabilidad de clase original: 1.537799835205078e-05\n",
      "tensor([[-3.6992,  4.1944]], device='cuda:0')\n",
      "[[3.7298136e-04 9.9962699e-01]]\n",
      "[3.7298136e-04 9.9962699e-01]\n",
      "Sin 'Tenacious': Clase 1, Probabilidades [3.7298136e-04 9.9962699e-01], Cambio en probabilidad de clase original: 1.0728836059570312e-05\n",
      "tensor([[-3.7012,  4.1888]], device='cuda:0')\n",
      "[[3.743525e-04 9.996257e-01]]\n",
      "[3.743525e-04 9.996257e-01]\n",
      "Sin 'D': Clase 1, Probabilidades [3.743525e-04 9.996257e-01], Cambio en probabilidad de clase original: 1.2040138244628906e-05\n",
      "tensor([[-3.6982,  4.1860]], device='cuda:0')\n",
      "[[3.7649376e-04 9.9962354e-01]]\n",
      "[3.7649376e-04 9.9962354e-01]\n",
      "Sin 'por': Clase 1, Probabilidades [3.7649376e-04 9.9962354e-01], Cambio en probabilidad de clase original: 1.4185905456542969e-05\n",
      "tensor([[-3.6961,  4.1832]], device='cuda:0')\n",
      "[[3.783673e-04 9.996216e-01]]\n",
      "[3.783673e-04 9.996216e-01]\n",
      "Sin 'su': Clase 1, Probabilidades [3.783673e-04 9.996216e-01], Cambio en probabilidad de clase original: 1.609325408935547e-05\n",
      "tensor([[-3.6924,  4.1771]], device='cuda:0')\n",
      "[[3.8209025e-04 9.9961793e-01]]\n",
      "[3.8209025e-04 9.9961793e-01]\n",
      "Sin 'primer': Clase 1, Probabilidades [3.8209025e-04 9.9961793e-01], Cambio en probabilidad de clase original: 1.9788742065429688e-05\n",
      "tensor([[-3.6934,  4.1754]], device='cuda:0')\n",
      "[[3.823677e-04 9.996176e-01]]\n",
      "[3.823677e-04 9.996176e-01]\n",
      "Sin '√°lbum': Clase 1, Probabilidades [3.823677e-04 9.996176e-01], Cambio en probabilidad de clase original: 2.014636993408203e-05\n",
      "tensor([[-3.6971,  4.1807]], device='cuda:0')\n",
      "[[3.7895748e-04 9.9962103e-01]]\n",
      "[3.7895748e-04 9.9962103e-01]\n",
      "Sin 'y': Clase 1, Probabilidades [3.7895748e-04 9.9962103e-01], Cambio en probabilidad de clase original: 1.6689300537109375e-05\n",
      "tensor([[-3.6987,  4.1844]], device='cuda:0')\n",
      "[[3.7694158e-04 9.9962306e-01]]\n",
      "[3.7694158e-04 9.9962306e-01]\n",
      "Sin ',': Clase 1, Probabilidades [3.7694158e-04 9.9962306e-01], Cambio en probabilidad de clase original: 1.4662742614746094e-05\n",
      "tensor([[-3.6974,  4.1846]], device='cuda:0')\n",
      "[[3.7730145e-04 9.9962270e-01]]\n",
      "[3.7730145e-04 9.9962270e-01]\n",
      "Sin 'naturalmente': Clase 1, Probabilidades [3.7730145e-04 9.9962270e-01], Cambio en probabilidad de clase original: 1.5020370483398438e-05\n",
      "tensor([[-3.6990,  4.1856]], device='cuda:0')\n",
      "[[3.7636564e-04 9.9962366e-01]]\n",
      "[3.7636564e-04 9.9962366e-01]\n",
      "Sin ',': Clase 1, Probabilidades [3.7636564e-04 9.9962366e-01], Cambio en probabilidad de clase original: 1.4066696166992188e-05\n",
      "tensor([[-3.6903,  4.1749]], device='cuda:0')\n",
      "[[3.8372559e-04 9.9961627e-01]]\n",
      "[3.8372559e-04 9.9961627e-01]\n",
      "Sin 'escuch√©': Clase 1, Probabilidades [3.8372559e-04 9.9961627e-01], Cambio en probabilidad de clase original: 2.1457672119140625e-05\n",
      "tensor([[-3.6989,  4.1859]], device='cuda:0')\n",
      "[[3.7626966e-04 9.9962366e-01]]\n",
      "[3.7626966e-04 9.9962366e-01]\n",
      "Sin 'algunas': Clase 1, Probabilidades [3.7626966e-04 9.9962366e-01], Cambio en probabilidad de clase original: 1.4066696166992188e-05\n",
      "tensor([[-3.6957,  4.1808]], device='cuda:0')\n",
      "[[3.7943359e-04 9.9962056e-01]]\n",
      "[3.7943359e-04 9.9962056e-01]\n",
      "Sin 'canciones': Clase 1, Probabilidades [3.7943359e-04 9.9962056e-01], Cambio en probabilidad de clase original: 1.71661376953125e-05\n",
      "tensor([[-3.6991,  4.1887]], device='cuda:0')\n",
      "[[3.7518886e-04 9.9962485e-01]]\n",
      "[3.7518886e-04 9.9962485e-01]\n",
      "Sin 'de': Clase 1, Probabilidades [3.7518886e-04 9.9962485e-01], Cambio en probabilidad de clase original: 1.2874603271484375e-05\n",
      "tensor([[-3.6990,  4.1869]], device='cuda:0')\n",
      "[[3.7588482e-04 9.9962413e-01]]\n",
      "[3.7588482e-04 9.9962413e-01]\n",
      "Sin 'The': Clase 1, Probabilidades [3.7588482e-04 9.9962413e-01], Cambio en probabilidad de clase original: 1.3589859008789062e-05\n",
      "tensor([[-3.7174,  4.2391]], device='cuda:0')\n",
      "[[3.5025360e-04 9.9964976e-01]]\n",
      "[3.5025360e-04 9.9964976e-01]\n",
      "Sin 'P.O.D.': Clase 1, Probabilidades [3.5025360e-04 9.9964976e-01], Cambio en probabilidad de clase original: -1.2040138244628906e-05\n",
      "tensor([[-3.6987,  4.1860]], device='cuda:0')\n",
      "[[3.7631413e-04 9.9962366e-01]]\n",
      "[3.7631413e-04 9.9962366e-01]\n",
      "Sin ',': Clase 1, Probabilidades [3.7631413e-04 9.9962366e-01], Cambio en probabilidad de clase original: 1.4066696166992188e-05\n",
      "tensor([[-3.7022,  4.1891]], device='cuda:0')\n",
      "[[3.7382112e-04 9.9962616e-01]]\n",
      "[3.7382112e-04 9.9962616e-01]\n",
      "Sin 'pero': Clase 1, Probabilidades [3.7382112e-04 9.9962616e-01], Cambio en probabilidad de clase original: 1.1563301086425781e-05\n",
      "tensor([[-3.7056,  4.2035]], device='cuda:0')\n",
      "[[3.672552e-04 9.996327e-01]]\n",
      "[3.672552e-04 9.996327e-01]\n",
      "Sin 'qued√©': Clase 1, Probabilidades [3.672552e-04 9.996327e-01], Cambio en probabilidad de clase original: 5.0067901611328125e-06\n",
      "tensor([[-3.7010,  4.1891]], device='cuda:0')\n",
      "[[3.7428236e-04 9.9962568e-01]]\n",
      "[3.7428236e-04 9.9962568e-01]\n",
      "Sin 'bastante': Clase 1, Probabilidades [3.7428236e-04 9.9962568e-01], Cambio en probabilidad de clase original: 1.2040138244628906e-05\n",
      "tensor([[-3.7038,  4.2224]], device='cuda:0')\n",
      "[[3.6101756e-04 9.9963892e-01]]\n",
      "[3.6101756e-04 9.9963892e-01]\n",
      "Sin 'decepcionado': Clase 1, Probabilidades [3.6101756e-04 9.9963892e-01], Cambio en probabilidad de clase original: -1.1920928955078125e-06\n",
      "tensor([[-3.7018,  4.1749]], device='cuda:0')\n",
      "[[3.793486e-04 9.996207e-01]]\n",
      "[3.793486e-04 9.996207e-01]\n",
      "Sin '.': Clase 1, Probabilidades [3.793486e-04 9.996207e-01], Cambio en probabilidad de clase original: 1.704692840576172e-05\n",
      "tensor([[-3.6960,  4.1795]], device='cuda:0')\n",
      "[[3.797687e-04 9.996202e-01]]\n",
      "[3.797687e-04 9.996202e-01]\n",
      "Sin 'Despu√©s': Clase 1, Probabilidades [3.797687e-04 9.996202e-01], Cambio en probabilidad de clase original: 1.7523765563964844e-05\n",
      "tensor([[-3.7005,  4.1928]], device='cuda:0')\n",
      "[[3.731062e-04 9.996269e-01]]\n",
      "[3.731062e-04 9.996269e-01]\n",
      "Sin 'de': Clase 1, Probabilidades [3.731062e-04 9.996269e-01], Cambio en probabilidad de clase original: 1.0848045349121094e-05\n",
      "tensor([[-3.7006,  4.1924]], device='cuda:0')\n",
      "[[3.7320046e-04 9.9962676e-01]]\n",
      "[3.7320046e-04 9.9962676e-01]\n",
      "Sin 'ver': Clase 1, Probabilidades [3.7320046e-04 9.9962676e-01], Cambio en probabilidad de clase original: 1.0967254638671875e-05\n",
      "tensor([[-3.7047,  4.1956]], device='cuda:0')\n",
      "[[3.7050492e-04 9.9962950e-01]]\n",
      "[3.7050492e-04 9.9962950e-01]\n",
      "Sin 'la': Clase 1, Probabilidades [3.7050492e-04 9.9962950e-01], Cambio en probabilidad de clase original: 8.225440979003906e-06\n",
      "tensor([[-3.7102,  4.2033]], device='cuda:0')\n",
      "[[3.6562938e-04 9.9963439e-01]]\n",
      "[3.6562938e-04 9.9963439e-01]\n",
      "Sin 'pel√≠cula': Clase 1, Probabilidades [3.6562938e-04 9.9963439e-01], Cambio en probabilidad de clase original: 3.337860107421875e-06\n",
      "tensor([[-3.7047,  4.1896]], device='cuda:0')\n",
      "[[3.7271957e-04 9.9962723e-01]]\n",
      "[3.7271957e-04 9.9962723e-01]\n",
      "Sin ',': Clase 1, Probabilidades [3.7271957e-04 9.9962723e-01], Cambio en probabilidad de clase original: 1.049041748046875e-05\n",
      "tensor([[-3.7004,  4.1861]], device='cuda:0')\n",
      "[[3.7566238e-04 9.9962437e-01]]\n",
      "[3.7566238e-04 9.9962437e-01]\n",
      "Sin 'mi': Clase 1, Probabilidades [3.7566238e-04 9.9962437e-01], Cambio en probabilidad de clase original: 1.33514404296875e-05\n",
      "tensor([[-3.6928,  4.1845]], device='cuda:0')\n",
      "[[3.7910856e-04 9.9962091e-01]]\n",
      "[3.7910856e-04 9.9962091e-01]\n",
      "Sin 'perspectiva': Clase 1, Probabilidades [3.7910856e-04 9.9962091e-01], Cambio en probabilidad de clase original: 1.6808509826660156e-05\n",
      "tensor([[-3.6990,  4.1933]], device='cuda:0')\n",
      "[[3.7347132e-04 9.9962652e-01]]\n",
      "[3.7347132e-04 9.9962652e-01]\n",
      "Sin 'cambi√≥': Clase 1, Probabilidades [3.7347132e-04 9.9962652e-01], Cambio en probabilidad de clase original: 1.1205673217773438e-05\n",
      "tensor([[-3.7130,  4.2065]], device='cuda:0')\n",
      "[[3.6345248e-04 9.9963653e-01]]\n",
      "[3.6345248e-04 9.9963653e-01]\n",
      "Sin '.': Clase 1, Probabilidades [3.6345248e-04 9.9963653e-01], Cambio en probabilidad de clase original: 1.1920928955078125e-06\n",
      "tensor([[-3.7084,  4.1975]], device='cuda:0')\n",
      "[[3.6843526e-04 9.9963152e-01]]\n",
      "[3.6843526e-04 9.9963152e-01]\n",
      "Sin 'La': Clase 1, Probabilidades [3.6843526e-04 9.9963152e-01], Cambio en probabilidad de clase original: 6.198883056640625e-06\n",
      "tensor([[-3.7104,  4.1896]], device='cuda:0')\n",
      "[[3.7063845e-04 9.9962938e-01]]\n",
      "[3.7063845e-04 9.9962938e-01]\n",
      "Sin 'pel√≠cula': Clase 1, Probabilidades [3.7063845e-04 9.9962938e-01], Cambio en probabilidad de clase original: 8.344650268554688e-06\n",
      "tensor([[-3.7012,  4.1917]], device='cuda:0')\n",
      "[[3.7326026e-04 9.9962676e-01]]\n",
      "[3.7326026e-04 9.9962676e-01]\n",
      "Sin 'es': Clase 1, Probabilidades [3.7326026e-04 9.9962676e-01], Cambio en probabilidad de clase original: 1.0967254638671875e-05\n",
      "tensor([[-3.6989,  4.1915]], device='cuda:0')\n",
      "[[3.741691e-04 9.996258e-01]]\n",
      "[3.741691e-04 9.996258e-01]\n",
      "Sin 'bastante': Clase 1, Probabilidades [3.741691e-04 9.996258e-01], Cambio en probabilidad de clase original: 1.1920928955078125e-05\n",
      "tensor([[-3.7186,  4.2077]], device='cuda:0')\n",
      "[[3.6099693e-04 9.9963903e-01]]\n",
      "[3.6099693e-04 9.9963903e-01]\n",
      "Sin 'divertida': Clase 1, Probabilidades [3.6099693e-04 9.9963903e-01], Cambio en probabilidad de clase original: -1.3113021850585938e-06\n",
      "tensor([[-3.6981,  4.1900]], device='cuda:0')\n",
      "[[3.7503545e-04 9.9962497e-01]]\n",
      "[3.7503545e-04 9.9962497e-01]\n",
      "Sin 'de': Clase 1, Probabilidades [3.7503545e-04 9.9962497e-01], Cambio en probabilidad de clase original: 1.2755393981933594e-05\n",
      "tensor([[-3.7028,  4.1929]], device='cuda:0')\n",
      "[[3.7221942e-04 9.9962771e-01]]\n",
      "[3.7221942e-04 9.9962771e-01]\n",
      "Sin 'principio': Clase 1, Probabilidades [3.7221942e-04 9.9962771e-01], Cambio en probabilidad de clase original: 1.0013580322265625e-05\n",
      "tensor([[-3.7102,  4.2028]], device='cuda:0')\n",
      "[[3.6583023e-04 9.9963415e-01]]\n",
      "[3.6583023e-04 9.9963415e-01]\n",
      "Sin 'a': Clase 1, Probabilidades [3.6583023e-04 9.9963415e-01], Cambio en probabilidad de clase original: 3.5762786865234375e-06\n",
      "tensor([[-3.7058,  4.2030]], device='cuda:0')\n",
      "[[3.6732992e-04 9.9963260e-01]]\n",
      "[3.6732992e-04 9.9963260e-01]\n",
      "Sin 'fin': Clase 1, Probabilidades [3.6732992e-04 9.9963260e-01], Cambio en probabilidad de clase original: 5.125999450683594e-06\n",
      "tensor([[-3.6997,  4.1931]], device='cuda:0')\n",
      "[[3.7329510e-04 9.9962664e-01]]\n",
      "[3.7329510e-04 9.9962664e-01]\n",
      "Sin ',': Clase 1, Probabilidades [3.7329510e-04 9.9962664e-01], Cambio en probabilidad de clase original: 1.1086463928222656e-05\n",
      "tensor([[-3.7037,  4.1979]], device='cuda:0')\n",
      "[[3.7002482e-04 9.9962997e-01]]\n",
      "[3.7002482e-04 9.9962997e-01]\n",
      "Sin 'y': Clase 1, Probabilidades [3.7002482e-04 9.9962997e-01], Cambio en probabilidad de clase original: 7.748603820800781e-06\n",
      "tensor([[-3.7164,  4.2180]], device='cuda:0')\n",
      "[[3.5807054e-04 9.9964190e-01]]\n",
      "[3.5807054e-04 9.9964190e-01]\n",
      "Sin 'me': Clase 1, Probabilidades [3.5807054e-04 9.9964190e-01], Cambio en probabilidad de clase original: -4.172325134277344e-06\n",
      "tensor([[-3.7070,  4.2032]], device='cuda:0')\n",
      "[[3.6685108e-04 9.9963319e-01]]\n",
      "[3.6685108e-04 9.9963319e-01]\n",
      "Sin 'encontr√©': Clase 1, Probabilidades [3.6685108e-04 9.9963319e-01], Cambio en probabilidad de clase original: 4.5299530029296875e-06\n",
      "tensor([[-3.6350,  4.1085]], device='cuda:0')\n",
      "[[4.3335295e-04 9.9956661e-01]]\n",
      "[4.3335295e-04 9.9956661e-01]\n",
      "Sin 'enganchado': Clase 1, Probabilidades [4.3335295e-04 9.9956661e-01], Cambio en probabilidad de clase original: 7.110834121704102e-05\n",
      "tensor([[-3.7100,  4.2035]], device='cuda:0')\n",
      "[[3.656165e-04 9.996344e-01]]\n",
      "[3.656165e-04 9.996344e-01]\n",
      "Sin 'a': Clase 1, Probabilidades [3.656165e-04 9.996344e-01], Cambio en probabilidad de clase original: 3.337860107421875e-06\n",
      "tensor([[-3.6692,  4.1458]], device='cuda:0')\n",
      "[[4.0346442e-04 9.9959654e-01]]\n",
      "[4.0346442e-04 9.9959654e-01]\n",
      "Sin 'pesar': Clase 1, Probabilidades [4.0346442e-04 9.9959654e-01], Cambio en probabilidad de clase original: 4.118680953979492e-05\n",
      "tensor([[-3.7008,  4.1921]], device='cuda:0')\n",
      "[[3.7324885e-04 9.9962676e-01]]\n",
      "[3.7324885e-04 9.9962676e-01]\n",
      "Sin 'de': Clase 1, Probabilidades [3.7324885e-04 9.9962676e-01], Cambio en probabilidad de clase original: 1.0967254638671875e-05\n",
      "tensor([[-3.7014,  4.1920]], device='cuda:0')\n",
      "[[3.7304038e-04 9.9962699e-01]]\n",
      "[3.7304038e-04 9.9962699e-01]\n",
      "Sin 'que': Clase 1, Probabilidades [3.7304038e-04 9.9962699e-01], Cambio en probabilidad de clase original: 1.0728836059570312e-05\n",
      "tensor([[-3.7012,  4.1961]], device='cuda:0')\n",
      "[[3.715972e-04 9.996284e-01]]\n",
      "[3.715972e-04 9.996284e-01]\n",
      "Sin 'la': Clase 1, Probabilidades [3.715972e-04 9.996284e-01], Cambio en probabilidad de clase original: 9.298324584960938e-06\n",
      "tensor([[-3.6926,  4.1802]], device='cuda:0')\n",
      "[[3.8079385e-04 9.9961913e-01]]\n",
      "[3.8079385e-04 9.9961913e-01]\n",
      "Sin 'trama': Clase 1, Probabilidades [3.8079385e-04 9.9961913e-01], Cambio en probabilidad de clase original: 1.8596649169921875e-05\n",
      "tensor([[-3.6989,  4.1919]], device='cuda:0')\n",
      "[[3.740339e-04 9.996259e-01]]\n",
      "[3.740339e-04 9.996259e-01]\n",
      "Sin 'era': Clase 1, Probabilidades [3.740339e-04 9.996259e-01], Cambio en probabilidad de clase original: 1.1801719665527344e-05\n",
      "tensor([[-3.7084,  4.2064]], device='cuda:0')\n",
      "[[3.6516279e-04 9.9963486e-01]]\n",
      "[3.6516279e-04 9.9963486e-01]\n",
      "Sin 'realmente': Clase 1, Probabilidades [3.6516279e-04 9.9963486e-01], Cambio en probabilidad de clase original: 2.86102294921875e-06\n",
      "tensor([[-3.6856,  4.1805]], device='cuda:0')\n",
      "[[3.833888e-04 9.996166e-01]]\n",
      "[3.833888e-04 9.996166e-01]\n",
      "Sin 'absurda': Clase 1, Probabilidades [3.833888e-04 9.996166e-01], Cambio en probabilidad de clase original: 2.110004425048828e-05\n",
      "tensor([[-3.6997,  4.1901]], device='cuda:0')\n",
      "[[3.7438527e-04 9.9962556e-01]]\n",
      "[3.7438527e-04 9.9962556e-01]\n",
      "Sin ',': Clase 1, Probabilidades [3.7438527e-04 9.9962556e-01], Cambio en probabilidad de clase original: 1.2159347534179688e-05\n",
      "tensor([[-3.7073,  4.1982]], device='cuda:0')\n",
      "[[3.685624e-04 9.996314e-01]]\n",
      "[3.685624e-04 9.996314e-01]\n",
      "Sin 'gracias': Clase 1, Probabilidades [3.685624e-04 9.996314e-01], Cambio en probabilidad de clase original: 6.318092346191406e-06\n",
      "tensor([[-3.7044,  4.2008]], device='cuda:0')\n",
      "[[3.6870225e-04 9.9963129e-01]]\n",
      "[3.6870225e-04 9.9963129e-01]\n",
      "Sin 'a': Clase 1, Probabilidades [3.6870225e-04 9.9963129e-01], Cambio en probabilidad de clase original: 6.4373016357421875e-06\n",
      "tensor([[-3.7024,  4.1971]], device='cuda:0')\n",
      "[[3.7080079e-04 9.9962914e-01]]\n",
      "[3.7080079e-04 9.9962914e-01]\n",
      "Sin 'las': Clase 1, Probabilidades [3.7080079e-04 9.9962914e-01], Cambio en probabilidad de clase original: 8.58306884765625e-06\n",
      "tensor([[-3.6973,  4.2033]], device='cuda:0')\n",
      "[[3.7038059e-04 9.9962962e-01]]\n",
      "[3.7038059e-04 9.9962962e-01]\n",
      "Sin 'actitudes': Clase 1, Probabilidades [3.7038059e-04 9.9962962e-01], Cambio en probabilidad de clase original: 8.106231689453125e-06\n",
      "tensor([[-3.6983,  4.1936]], device='cuda:0')\n",
      "[[3.736266e-04 9.996264e-01]]\n",
      "[3.736266e-04 9.996264e-01]\n",
      "Sin 'que': Clase 1, Probabilidades [3.736266e-04 9.996264e-01], Cambio en probabilidad de clase original: 1.1324882507324219e-05\n",
      "tensor([[-3.6960,  4.1921]], device='cuda:0')\n",
      "[[3.7506493e-04 9.9962497e-01]]\n",
      "[3.7506493e-04 9.9962497e-01]\n",
      "Sin 'KG': Clase 1, Probabilidades [3.7506493e-04 9.9962497e-01], Cambio en probabilidad de clase original: 1.2755393981933594e-05\n",
      "tensor([[-3.7005,  4.1919]], device='cuda:0')\n",
      "[[3.7343925e-04 9.9962652e-01]]\n",
      "[3.7343925e-04 9.9962652e-01]\n",
      "Sin 'y': Clase 1, Probabilidades [3.7343925e-04 9.9962652e-01], Cambio en probabilidad de clase original: 1.1205673217773438e-05\n",
      "tensor([[-3.6880,  4.1880]], device='cuda:0')\n",
      "[[3.796119e-04 9.996203e-01]]\n",
      "[3.796119e-04 9.996203e-01]\n",
      "Sin 'Jaybles': Clase 1, Probabilidades [3.796119e-04 9.996203e-01], Cambio en probabilidad de clase original: 1.7404556274414062e-05\n",
      "tensor([[-3.6848,  4.1797]], device='cuda:0')\n",
      "[[3.8396893e-04 9.9961603e-01]]\n",
      "[3.8396893e-04 9.9961603e-01]\n",
      "Sin 'retratan': Clase 1, Probabilidades [3.8396893e-04 9.9961603e-01], Cambio en probabilidad de clase original: 2.1696090698242188e-05\n",
      "tensor([[-3.7013,  4.1984]], device='cuda:0')\n",
      "[[3.7073420e-04 9.9962926e-01]]\n",
      "[3.7073420e-04 9.9962926e-01]\n",
      "Sin 'en': Clase 1, Probabilidades [3.7073420e-04 9.9962926e-01], Cambio en probabilidad de clase original: 8.463859558105469e-06\n",
      "tensor([[-3.7007,  4.1951]], device='cuda:0')\n",
      "[[3.721623e-04 9.996278e-01]]\n",
      "[3.721623e-04 9.996278e-01]\n",
      "Sin 'la': Clase 1, Probabilidades [3.721623e-04 9.996278e-01], Cambio en probabilidad de clase original: 9.894371032714844e-06\n",
      "tensor([[-3.7087,  4.2023]], device='cuda:0')\n",
      "[[3.6654586e-04 9.9963343e-01]]\n",
      "[3.6654586e-04 9.9963343e-01]\n",
      "Sin 'pel√≠cula': Clase 1, Probabilidades [3.6654586e-04 9.9963343e-01], Cambio en probabilidad de clase original: 4.291534423828125e-06\n",
      "tensor([[-3.6881,  4.1762]], device='cuda:0')\n",
      "[[3.8405677e-04 9.9961591e-01]]\n",
      "[3.8405677e-04 9.9961591e-01]\n",
      "Sin '.': Clase 1, Probabilidades [3.8405677e-04 9.9961591e-01], Cambio en probabilidad de clase original: 2.181529998779297e-05\n",
      "tensor([[-3.7059,  4.1935]], device='cuda:0')\n",
      "[[3.7084342e-04 9.9962914e-01]]\n",
      "[3.7084342e-04 9.9962914e-01]\n",
      "Sin 'Mucho': Clase 1, Probabilidades [3.7084342e-04 9.9962914e-01], Cambio en probabilidad de clase original: 8.58306884765625e-06\n",
      "tensor([[-3.7198,  4.2549]], device='cuda:0')\n",
      "[[3.4396144e-04 9.9965608e-01]]\n",
      "[3.4396144e-04 9.9965608e-01]\n",
      "Sin 'm√°s': Clase 1, Probabilidades [3.4396144e-04 9.9965608e-01], Cambio en probabilidad de clase original: -1.8358230590820312e-05\n",
      "tensor([[-3.7177,  4.2542]], device='cuda:0')\n",
      "[[3.4490615e-04 9.9965513e-01]]\n",
      "[3.4490615e-04 9.9965513e-01]\n",
      "Sin 'entretenida': Clase 1, Probabilidades [3.4490615e-04 9.9965513e-01], Cambio en probabilidad de clase original: -1.7404556274414062e-05\n",
      "tensor([[-3.7124,  4.2021]], device='cuda:0')\n",
      "[[3.6525208e-04 9.9963474e-01]]\n",
      "[3.6525208e-04 9.9963474e-01]\n",
      "Sin 'y': Clase 1, Probabilidades [3.6525208e-04 9.9963474e-01], Cambio en probabilidad de clase original: 2.9802322387695312e-06\n",
      "tensor([[-3.7186,  4.2307]], device='cuda:0')\n",
      "[[3.5277425e-04 9.9964726e-01]]\n",
      "[3.5277425e-04 9.9964726e-01]\n",
      "Sin 'agradable': Clase 1, Probabilidades [3.5277425e-04 9.9964726e-01], Cambio en probabilidad de clase original: -9.5367431640625e-06\n",
      "tensor([[-3.7322,  4.2379]], device='cuda:0')\n",
      "[[3.4550278e-04 9.9965453e-01]]\n",
      "[3.4550278e-04 9.9965453e-01]\n",
      "Sin 'que': Clase 1, Probabilidades [3.4550278e-04 9.9965453e-01], Cambio en probabilidad de clase original: -1.6808509826660156e-05\n",
      "tensor([[-3.7114,  4.2127]], device='cuda:0')\n",
      "[[3.618059e-04 9.996382e-01]]\n",
      "[3.618059e-04 9.996382e-01]\n",
      "Sin 'otras': Clase 1, Probabilidades [3.618059e-04 9.996382e-01], Cambio en probabilidad de clase original: -4.76837158203125e-07\n",
      "tensor([[-3.7033,  4.2008]], device='cuda:0')\n",
      "[[3.6907432e-04 9.9963093e-01]]\n",
      "[3.6907432e-04 9.9963093e-01]\n",
      "Sin 'pel√≠culas': Clase 1, Probabilidades [3.6907432e-04 9.9963093e-01], Cambio en probabilidad de clase original: 6.794929504394531e-06\n",
      "tensor([[-3.7213,  4.2309]], device='cuda:0')\n",
      "[[3.5177078e-04 9.9964821e-01]]\n",
      "[3.5177078e-04 9.9964821e-01]\n",
      "Sin 'que': Clase 1, Probabilidades [3.5177078e-04 9.9964821e-01], Cambio en probabilidad de clase original: -1.049041748046875e-05\n",
      "tensor([[-3.7075,  4.2064]], device='cuda:0')\n",
      "[[3.6547988e-04 9.9963450e-01]]\n",
      "[3.6547988e-04 9.9963450e-01]\n",
      "Sin 'he': Clase 1, Probabilidades [3.6547988e-04 9.9963450e-01], Cambio en probabilidad de clase original: 3.2186508178710938e-06\n",
      "tensor([[-3.7126,  4.2065]], device='cuda:0')\n",
      "[[3.6360256e-04 9.9963641e-01]]\n",
      "[3.6360256e-04 9.9963641e-01]\n",
      "Sin 'visto': Clase 1, Probabilidades [3.6360256e-04 9.9963641e-01], Cambio en probabilidad de clase original: 1.3113021850585938e-06\n",
      "tensor([[-3.7093,  4.2100]], device='cuda:0')\n",
      "[[3.635261e-04 9.996364e-01]]\n",
      "[3.635261e-04 9.996364e-01]\n",
      "Sin 'en': Clase 1, Probabilidades [3.635261e-04 9.996364e-01], Cambio en probabilidad de clase original: 1.3113021850585938e-06\n",
      "tensor([[-3.7130,  4.2168]], device='cuda:0')\n",
      "[[3.5975326e-04 9.9964023e-01]]\n",
      "[3.5975326e-04 9.9964023e-01]\n",
      "Sin 'el': Clase 1, Probabilidades [3.5975326e-04 9.9964023e-01], Cambio en probabilidad de clase original: -2.5033950805664062e-06\n",
      "tensor([[-3.7079,  4.2083]], device='cuda:0')\n",
      "[[3.6463945e-04 9.9963534e-01]]\n",
      "[3.6463945e-04 9.9963534e-01]\n",
      "Sin 'cine': Clase 1, Probabilidades [3.6463945e-04 9.9963534e-01], Cambio en probabilidad de clase original: 2.384185791015625e-06\n",
      "tensor([[-3.6908,  4.1845]], device='cuda:0')\n",
      "[[3.798726e-04 9.996201e-01]]\n",
      "[3.798726e-04 9.996201e-01]\n",
      "Sin '√∫ltimamente': Clase 1, Probabilidades [3.798726e-04 9.996201e-01], Cambio en probabilidad de clase original: 1.7642974853515625e-05\n",
      "tensor([[-3.7090,  4.2089]], device='cuda:0')\n",
      "[[3.6402603e-04 9.9963593e-01]]\n",
      "[3.6402603e-04 9.9963593e-01]\n",
      "Sin ',': Clase 1, Probabilidades [3.6402603e-04 9.9963593e-01], Cambio en probabilidad de clase original: 1.7881393432617188e-06\n",
      "tensor([[-3.7091,  4.2074]], device='cuda:0')\n",
      "[[3.6456450e-04 9.9963546e-01]]\n",
      "[3.6456450e-04 9.9963546e-01]\n",
      "Sin 'como': Clase 1, Probabilidades [3.6456450e-04 9.9963546e-01], Cambio en probabilidad de clase original: 2.2649765014648438e-06\n",
      "tensor([[-3.7072,  4.2150]], device='cuda:0')\n",
      "[[3.624729e-04 9.996375e-01]]\n",
      "[3.624729e-04 9.996375e-01]\n",
      "Sin 'Saw': Clase 1, Probabilidades [3.624729e-04 9.996375e-01], Cambio en probabilidad de clase original: 2.384185791015625e-07\n",
      "tensor([[-3.7218,  4.2314]], device='cuda:0')\n",
      "[[3.514148e-04 9.996486e-01]]\n",
      "[3.514148e-04 9.996486e-01]\n",
      "Sin 'III': Clase 1, Probabilidades [3.514148e-04 9.996486e-01], Cambio en probabilidad de clase original: -1.0848045349121094e-05\n",
      "tensor([[-3.7135,  4.2172]], device='cuda:0')\n",
      "[[3.594139e-04 9.996406e-01]]\n",
      "[3.594139e-04 9.996406e-01]\n",
      "Sin '(': Clase 1, Probabilidades [3.594139e-04 9.996406e-01], Cambio en probabilidad de clase original: -2.86102294921875e-06\n",
      "tensor([[-3.7127,  4.2168]], device='cuda:0')\n",
      "[[3.5983452e-04 9.9964011e-01]]\n",
      "[3.5983452e-04 9.9964011e-01]\n",
      "Sin 'aburrida': Clase 1, Probabilidades [3.5983452e-04 9.9964011e-01], Cambio en probabilidad de clase original: -2.384185791015625e-06\n",
      "tensor([[-3.7110,  4.2136]], device='cuda:0')\n",
      "[[3.6161518e-04 9.9963832e-01]]\n",
      "[3.6161518e-04 9.9963832e-01]\n",
      "Sin 'y': Clase 1, Probabilidades [3.6161518e-04 9.9963832e-01], Cambio en probabilidad de clase original: -5.960464477539062e-07\n",
      "tensor([[-3.7125,  4.2157]], device='cuda:0')\n",
      "[[3.6028321e-04 9.9963975e-01]]\n",
      "[3.6028321e-04 9.9963975e-01]\n",
      "Sin 'lenta': Clase 1, Probabilidades [3.6028321e-04 9.9963975e-01], Cambio en probabilidad de clase original: -2.0265579223632812e-06\n",
      "tensor([[-3.7135,  4.2155]], device='cuda:0')\n",
      "[[3.6002637e-04 9.9963999e-01]]\n",
      "[3.6002637e-04 9.9963999e-01]\n",
      "Sin ')': Clase 1, Probabilidades [3.6002637e-04 9.9963999e-01], Cambio en probabilidad de clase original: -2.2649765014648438e-06\n",
      "tensor([[-3.7140,  4.2161]], device='cuda:0')\n",
      "[[3.5960550e-04 9.9964035e-01]]\n",
      "[3.5960550e-04 9.9964035e-01]\n",
      "Sin 'o': Clase 1, Probabilidades [3.5960550e-04 9.9964035e-01], Cambio en probabilidad de clase original: -2.6226043701171875e-06\n",
      "tensor([[-3.7106,  4.2178]], device='cuda:0')\n",
      "[[3.6024163e-04 9.9963975e-01]]\n",
      "[3.6024163e-04 9.9963975e-01]\n",
      "Sin 'Casino': Clase 1, Probabilidades [3.6024163e-04 9.9963975e-01], Cambio en probabilidad de clase original: -2.0265579223632812e-06\n",
      "tensor([[-3.7123,  4.2195]], device='cuda:0')\n",
      "[[3.5899319e-04 9.9964094e-01]]\n",
      "[3.5899319e-04 9.9964094e-01]\n",
      "Sin 'Royale': Clase 1, Probabilidades [3.5899319e-04 9.9964094e-01], Cambio en probabilidad de clase original: -3.2186508178710938e-06\n",
      "tensor([[-3.7097,  4.2120]], device='cuda:0')\n",
      "[[3.6267133e-04 9.9963737e-01]]\n",
      "[3.6267133e-04 9.9963737e-01]\n",
      "Sin '(': Clase 1, Probabilidades [3.6267133e-04 9.9963737e-01], Cambio en probabilidad de clase original: 3.5762786865234375e-07\n",
      "tensor([[-3.7100,  4.2127]], device='cuda:0')\n",
      "[[3.622824e-04 9.996377e-01]]\n",
      "[3.622824e-04 9.996377e-01]\n",
      "Sin 'demasiado': Clase 1, Probabilidades [3.622824e-04 9.996377e-01], Cambio en probabilidad de clase original: 0.0\n",
      "tensor([[-3.7135,  4.2246]], device='cuda:0')\n",
      "[[3.5676514e-04 9.9964321e-01]]\n",
      "[3.5676514e-04 9.9964321e-01]\n",
      "Sin 'homoer√≥tica': Clase 1, Probabilidades [3.5676514e-04 9.9964321e-01], Cambio en probabilidad de clase original: -5.4836273193359375e-06\n",
      "tensor([[-3.7131,  4.2160]], device='cuda:0')\n",
      "[[3.5998123e-04 9.9963999e-01]]\n",
      "[3.5998123e-04 9.9963999e-01]\n",
      "Sin ')': Clase 1, Probabilidades [3.5998123e-04 9.9963999e-01], Cambio en probabilidad de clase original: -2.2649765014648438e-06\n",
      "tensor([[-3.7127,  4.2145]], device='cuda:0')\n",
      "[[3.6067632e-04 9.9963927e-01]]\n",
      "[3.6067632e-04 9.9963927e-01]\n",
      "Sin ',': Clase 1, Probabilidades [3.6067632e-04 9.9963927e-01], Cambio en probabilidad de clase original: -1.5497207641601562e-06\n",
      "tensor([[-3.7208,  4.2092]], device='cuda:0')\n",
      "[[3.5966892e-04 9.9964035e-01]]\n",
      "[3.5966892e-04 9.9964035e-01]\n",
      "Sin 'aunque': Clase 1, Probabilidades [3.5966892e-04 9.9964035e-01], Cambio en probabilidad de clase original: -2.6226043701171875e-06\n",
      "tensor([[-3.7136,  4.2165]], device='cuda:0')\n",
      "[[3.5961985e-04 9.9964035e-01]]\n",
      "[3.5961985e-04 9.9964035e-01]\n",
      "Sin 'he': Clase 1, Probabilidades [3.5961985e-04 9.9964035e-01], Cambio en probabilidad de clase original: -2.6226043701171875e-06\n",
      "tensor([[-3.7135,  4.2173]], device='cuda:0')\n",
      "[[3.593546e-04 9.996406e-01]]\n",
      "[3.593546e-04 9.996406e-01]\n",
      "Sin 'disfrutado': Clase 1, Probabilidades [3.593546e-04 9.996406e-01], Cambio en probabilidad de clase original: -2.86102294921875e-06\n",
      "tensor([[-3.7102,  4.2133]], device='cuda:0')\n",
      "[[3.6200046e-04 9.9963796e-01]]\n",
      "[3.6200046e-04 9.9963796e-01]\n",
      "Sin 'mucho': Clase 1, Probabilidades [3.6200046e-04 9.9963796e-01], Cambio en probabilidad de clase original: -2.384185791015625e-07\n",
      "tensor([[-3.7119,  4.2130]], device='cuda:0')\n",
      "[[3.6146372e-04 9.9963856e-01]]\n",
      "[3.6146372e-04 9.9963856e-01]\n",
      "Sin 'entregas': Clase 1, Probabilidades [3.6146372e-04 9.9963856e-01], Cambio en probabilidad de clase original: -8.344650268554688e-07\n",
      "tensor([[-3.6971,  4.1980]], device='cuda:0')\n",
      "[[3.7240933e-04 9.9962759e-01]]\n",
      "[3.7240933e-04 9.9962759e-01]\n",
      "Sin 'anteriores': Clase 1, Probabilidades [3.7240933e-04 9.9962759e-01], Cambio en probabilidad de clase original: 1.0132789611816406e-05\n",
      "tensor([[-3.7161,  4.2192]], device='cuda:0')\n",
      "[[3.5773034e-04 9.9964225e-01]]\n",
      "[3.5773034e-04 9.9964225e-01]\n",
      "Sin '.': Clase 1, Probabilidades [3.5773034e-04 9.9964225e-01], Cambio en probabilidad de clase original: -4.5299530029296875e-06\n",
      "tensor([[-3.7179,  4.2180]], device='cuda:0')\n",
      "[[3.5754163e-04 9.9964249e-01]]\n",
      "[3.5754163e-04 9.9964249e-01]\n",
      "Sin 'Si': Clase 1, Probabilidades [3.5754163e-04 9.9964249e-01], Cambio en probabilidad de clase original: -4.76837158203125e-06\n",
      "tensor([[-3.7153,  4.2225]], device='cuda:0')\n",
      "[[3.568709e-04 9.996431e-01]]\n",
      "[3.568709e-04 9.996431e-01]\n",
      "Sin 'disfrutaste': Clase 1, Probabilidades [3.568709e-04 9.996431e-01], Cambio en probabilidad de clase original: -5.364418029785156e-06\n",
      "tensor([[-3.7094,  4.2230]], device='cuda:0')\n",
      "[[3.5877028e-04 9.9964118e-01]]\n",
      "[3.5877028e-04 9.9964118e-01]\n",
      "Sin 'Borat': Clase 1, Probabilidades [3.5877028e-04 9.9964118e-01], Cambio en probabilidad de clase original: -3.4570693969726562e-06\n",
      "tensor([[-3.7146,  4.2158]], device='cuda:0')\n",
      "[[3.5953760e-04 9.9964046e-01]]\n",
      "[3.5953760e-04 9.9964046e-01]\n",
      "Sin ',': Clase 1, Probabilidades [3.5953760e-04 9.9964046e-01], Cambio en probabilidad de clase original: -2.7418136596679688e-06\n",
      "tensor([[-3.7135,  4.2174]], device='cuda:0')\n",
      "[[3.5932893e-04 9.9964070e-01]]\n",
      "[3.5932893e-04 9.9964070e-01]\n",
      "Sin 'seguramente': Clase 1, Probabilidades [3.5932893e-04 9.9964070e-01], Cambio en probabilidad de clase original: -2.9802322387695312e-06\n",
      "tensor([[-3.7071,  4.2089]], device='cuda:0')\n",
      "[[3.6472926e-04 9.9963522e-01]]\n",
      "[3.6472926e-04 9.9963522e-01]\n",
      "Sin 'disfrutar√°s': Clase 1, Probabilidades [3.6472926e-04 9.9963522e-01], Cambio en probabilidad de clase original: 2.5033950805664062e-06\n",
      "tensor([[-3.7135,  4.2178]], device='cuda:0')\n",
      "[[3.5919333e-04 9.9964082e-01]]\n",
      "[3.5919333e-04 9.9964082e-01]\n",
      "Sin 'la': Clase 1, Probabilidades [3.5919333e-04 9.9964082e-01], Cambio en probabilidad de clase original: -3.0994415283203125e-06\n",
      "tensor([[-3.7062,  4.2099]], device='cuda:0')\n",
      "[[3.6468392e-04 9.9963534e-01]]\n",
      "[3.6468392e-04 9.9963534e-01]\n",
      "Sin 'historia': Clase 1, Probabilidades [3.6468392e-04 9.9963534e-01], Cambio en probabilidad de clase original: 2.384185791015625e-06\n",
      "tensor([[-3.7065,  4.2119]], device='cuda:0')\n",
      "[[3.6382498e-04 9.9963617e-01]]\n",
      "[3.6382498e-04 9.9963617e-01]\n",
      "Sin 'de': Clase 1, Probabilidades [3.6382498e-04 9.9963617e-01], Cambio en probabilidad de clase original: 1.5497207641601562e-06\n",
      "tensor([[-3.7114,  4.2144]], device='cuda:0')\n",
      "[[3.6114352e-04 9.9963880e-01]]\n",
      "[3.6114352e-04 9.9963880e-01]\n",
      "Sin 'The': Clase 1, Probabilidades [3.6114352e-04 9.9963880e-01], Cambio en probabilidad de clase original: -1.0728836059570312e-06\n",
      "tensor([[-3.7100,  4.2127]], device='cuda:0')\n",
      "[[3.622822e-04 9.996377e-01]]\n",
      "[3.622822e-04 9.996377e-01]\n",
      "Sin 'Greatest': Clase 1, Probabilidades [3.622822e-04 9.996377e-01], Cambio en probabilidad de clase original: 0.0\n",
      "tensor([[-3.7110,  4.2135]], device='cuda:0')\n",
      "[[3.6164658e-04 9.9963832e-01]]\n",
      "[3.6164658e-04 9.9963832e-01]\n",
      "Sin 'Band': Clase 1, Probabilidades [3.6164658e-04 9.9963832e-01], Cambio en probabilidad de clase original: -5.960464477539062e-07\n",
      "tensor([[-3.7112,  4.2135]], device='cuda:0')\n",
      "[[3.6155659e-04 9.9963844e-01]]\n",
      "[3.6155659e-04 9.9963844e-01]\n",
      "Sin 'on': Clase 1, Probabilidades [3.6155659e-04 9.9963844e-01], Cambio en probabilidad de clase original: -7.152557373046875e-07\n",
      "tensor([[-3.7185,  4.2253]], device='cuda:0')\n",
      "[[3.5472689e-04 9.9964523e-01]]\n",
      "[3.5472689e-04 9.9964523e-01]\n",
      "Sin 'Earth': Clase 1, Probabilidades [3.5472689e-04 9.9964523e-01], Cambio en probabilidad de clase original: -7.510185241699219e-06\n",
      "tensor([[-3.7157,  4.2343]], device='cuda:0')\n",
      "[[3.5256174e-04 9.9964738e-01]]\n",
      "[3.5256174e-04 9.9964738e-01]\n",
      "Sin '.': Clase 1, Probabilidades [3.5256174e-04 9.9964738e-01], Cambio en probabilidad de clase original: -9.655952453613281e-06\n",
      "\n",
      "Palabras ordenadas por impacto en la predicci√≥n de la clase original:\n",
      "Palabra 'enganchado': Cambio en probabilidad de clase original 0.00007111, Nueva clase: 1\n",
      "Palabra 'pesar': Cambio en probabilidad de clase original 0.00004119, Nueva clase: 1\n",
      "Palabra '.': Cambio en probabilidad de clase original 0.00002182, Nueva clase: 1\n",
      "Palabra 'retratan': Cambio en probabilidad de clase original 0.00002170, Nueva clase: 1\n",
      "Palabra 'escuch√©': Cambio en probabilidad de clase original 0.00002146, Nueva clase: 1\n",
      "Palabra 'absurda': Cambio en probabilidad de clase original 0.00002110, Nueva clase: 1\n",
      "Palabra '√°lbum': Cambio en probabilidad de clase original 0.00002015, Nueva clase: 1\n",
      "Palabra 'primer': Cambio en probabilidad de clase original 0.00001979, Nueva clase: 1\n",
      "Palabra 'trama': Cambio en probabilidad de clase original 0.00001860, Nueva clase: 1\n",
      "Palabra 'm√°s': Cambio en probabilidad de clase original -0.00001836, Nueva clase: 1\n",
      "Palabra '√∫ltimamente': Cambio en probabilidad de clase original 0.00001764, Nueva clase: 1\n",
      "Palabra 'Despu√©s': Cambio en probabilidad de clase original 0.00001752, Nueva clase: 1\n",
      "Palabra 'Jaybles': Cambio en probabilidad de clase original 0.00001740, Nueva clase: 1\n",
      "Palabra 'entretenida': Cambio en probabilidad de clase original -0.00001740, Nueva clase: 1\n",
      "Palabra 'yo': Cambio en probabilidad de clase original 0.00001717, Nueva clase: 1\n",
      "Palabra 'canciones': Cambio en probabilidad de clase original 0.00001717, Nueva clase: 1\n",
      "Palabra '.': Cambio en probabilidad de clase original 0.00001705, Nueva clase: 1\n",
      "Palabra 'perspectiva': Cambio en probabilidad de clase original 0.00001681, Nueva clase: 1\n",
      "Palabra 'que': Cambio en probabilidad de clase original -0.00001681, Nueva clase: 1\n",
      "Palabra 'Originalmente': Cambio en probabilidad de clase original 0.00001669, Nueva clase: 1\n",
      "Palabra 'y': Cambio en probabilidad de clase original 0.00001669, Nueva clase: 1\n",
      "Palabra 'era': Cambio en probabilidad de clase original 0.00001657, Nueva clase: 1\n",
      "Palabra 'su': Cambio en probabilidad de clase original 0.00001609, Nueva clase: 1\n",
      "Palabra 'de': Cambio en probabilidad de clase original 0.00001538, Nueva clase: 1\n",
      "Palabra 'naturalmente': Cambio en probabilidad de clase original 0.00001502, Nueva clase: 1\n",
      "Palabra ',': Cambio en probabilidad de clase original 0.00001466, Nueva clase: 1\n",
      "Palabra 'por': Cambio en probabilidad de clase original 0.00001419, Nueva clase: 1\n",
      "Palabra ',': Cambio en probabilidad de clase original 0.00001407, Nueva clase: 1\n",
      "Palabra 'algunas': Cambio en probabilidad de clase original 0.00001407, Nueva clase: 1\n",
      "Palabra ',': Cambio en probabilidad de clase original 0.00001407, Nueva clase: 1\n",
      "Palabra 'fan√°tico': Cambio en probabilidad de clase original 0.00001383, Nueva clase: 1\n",
      "Palabra 'The': Cambio en probabilidad de clase original 0.00001359, Nueva clase: 1\n",
      "Palabra 'mi': Cambio en probabilidad de clase original 0.00001335, Nueva clase: 1\n",
      "Palabra 'de': Cambio en probabilidad de clase original 0.00001287, Nueva clase: 1\n",
      "Palabra 'de': Cambio en probabilidad de clase original 0.00001276, Nueva clase: 1\n",
      "Palabra 'KG': Cambio en probabilidad de clase original 0.00001276, Nueva clase: 1\n",
      "Palabra ',': Cambio en probabilidad de clase original 0.00001216, Nueva clase: 1\n",
      "Palabra 'D': Cambio en probabilidad de clase original 0.00001204, Nueva clase: 1\n",
      "Palabra 'P.O.D.': Cambio en probabilidad de clase original -0.00001204, Nueva clase: 1\n",
      "Palabra 'bastante': Cambio en probabilidad de clase original 0.00001204, Nueva clase: 1\n",
      "Palabra 'bastante': Cambio en probabilidad de clase original 0.00001192, Nueva clase: 1\n",
      "Palabra 'era': Cambio en probabilidad de clase original 0.00001180, Nueva clase: 1\n",
      "Palabra ',': Cambio en probabilidad de clase original 0.00001156, Nueva clase: 1\n",
      "Palabra 'pero': Cambio en probabilidad de clase original 0.00001156, Nueva clase: 1\n",
      "Palabra 'que': Cambio en probabilidad de clase original 0.00001132, Nueva clase: 1\n",
      "Palabra 'cambi√≥': Cambio en probabilidad de clase original 0.00001121, Nueva clase: 1\n",
      "Palabra 'y': Cambio en probabilidad de clase original 0.00001121, Nueva clase: 1\n",
      "Palabra ',': Cambio en probabilidad de clase original 0.00001109, Nueva clase: 1\n",
      "Palabra 'ver': Cambio en probabilidad de clase original 0.00001097, Nueva clase: 1\n",
      "Palabra 'es': Cambio en probabilidad de clase original 0.00001097, Nueva clase: 1\n",
      "Palabra 'de': Cambio en probabilidad de clase original 0.00001097, Nueva clase: 1\n",
      "Palabra 'de': Cambio en probabilidad de clase original 0.00001085, Nueva clase: 1\n",
      "Palabra 'III': Cambio en probabilidad de clase original -0.00001085, Nueva clase: 1\n",
      "Palabra 'Tenacious': Cambio en probabilidad de clase original 0.00001073, Nueva clase: 1\n",
      "Palabra 'que': Cambio en probabilidad de clase original 0.00001073, Nueva clase: 1\n",
      "Palabra ',': Cambio en probabilidad de clase original 0.00001049, Nueva clase: 1\n",
      "Palabra 'que': Cambio en probabilidad de clase original -0.00001049, Nueva clase: 1\n",
      "Palabra 'anteriores': Cambio en probabilidad de clase original 0.00001013, Nueva clase: 1\n",
      "Palabra 'principio': Cambio en probabilidad de clase original 0.00001001, Nueva clase: 1\n",
      "Palabra 'la': Cambio en probabilidad de clase original 0.00000989, Nueva clase: 1\n",
      "Palabra '.': Cambio en probabilidad de clase original -0.00000966, Nueva clase: 1\n",
      "Palabra 'agradable': Cambio en probabilidad de clase original -0.00000954, Nueva clase: 1\n",
      "Palabra 'la': Cambio en probabilidad de clase original 0.00000930, Nueva clase: 1\n",
      "Palabra 'las': Cambio en probabilidad de clase original 0.00000858, Nueva clase: 1\n",
      "Palabra 'Mucho': Cambio en probabilidad de clase original 0.00000858, Nueva clase: 1\n",
      "Palabra 'en': Cambio en probabilidad de clase original 0.00000846, Nueva clase: 1\n",
      "Palabra 'pel√≠cula': Cambio en probabilidad de clase original 0.00000834, Nueva clase: 1\n",
      "Palabra 'la': Cambio en probabilidad de clase original 0.00000823, Nueva clase: 1\n",
      "Palabra 'actitudes': Cambio en probabilidad de clase original 0.00000811, Nueva clase: 1\n",
      "Palabra 'y': Cambio en probabilidad de clase original 0.00000775, Nueva clase: 1\n",
      "Palabra 'Earth': Cambio en probabilidad de clase original -0.00000751, Nueva clase: 1\n",
      "Palabra 'pel√≠culas': Cambio en probabilidad de clase original 0.00000679, Nueva clase: 1\n",
      "Palabra 'a': Cambio en probabilidad de clase original 0.00000644, Nueva clase: 1\n",
      "Palabra 'gracias': Cambio en probabilidad de clase original 0.00000632, Nueva clase: 1\n",
      "Palabra 'La': Cambio en probabilidad de clase original 0.00000620, Nueva clase: 1\n",
      "Palabra 'homoer√≥tica': Cambio en probabilidad de clase original -0.00000548, Nueva clase: 1\n",
      "Palabra 'disfrutaste': Cambio en probabilidad de clase original -0.00000536, Nueva clase: 1\n",
      "Palabra 'fin': Cambio en probabilidad de clase original 0.00000513, Nueva clase: 1\n",
      "Palabra 'qued√©': Cambio en probabilidad de clase original 0.00000501, Nueva clase: 1\n",
      "Palabra 'Si': Cambio en probabilidad de clase original -0.00000477, Nueva clase: 1\n",
      "Palabra 'encontr√©': Cambio en probabilidad de clase original 0.00000453, Nueva clase: 1\n",
      "Palabra '.': Cambio en probabilidad de clase original -0.00000453, Nueva clase: 1\n",
      "Palabra 'pel√≠cula': Cambio en probabilidad de clase original 0.00000429, Nueva clase: 1\n",
      "Palabra 'me': Cambio en probabilidad de clase original -0.00000417, Nueva clase: 1\n",
      "Palabra 'a': Cambio en probabilidad de clase original 0.00000358, Nueva clase: 1\n",
      "Palabra 'Borat': Cambio en probabilidad de clase original -0.00000346, Nueva clase: 1\n",
      "Palabra 'pel√≠cula': Cambio en probabilidad de clase original 0.00000334, Nueva clase: 1\n",
      "Palabra 'a': Cambio en probabilidad de clase original 0.00000334, Nueva clase: 1\n",
      "Palabra 'he': Cambio en probabilidad de clase original 0.00000322, Nueva clase: 1\n",
      "Palabra 'Royale': Cambio en probabilidad de clase original -0.00000322, Nueva clase: 1\n",
      "Palabra 'la': Cambio en probabilidad de clase original -0.00000310, Nueva clase: 1\n",
      "Palabra 'y': Cambio en probabilidad de clase original 0.00000298, Nueva clase: 1\n",
      "Palabra 'seguramente': Cambio en probabilidad de clase original -0.00000298, Nueva clase: 1\n",
      "Palabra 'realmente': Cambio en probabilidad de clase original 0.00000286, Nueva clase: 1\n",
      "Palabra '(': Cambio en probabilidad de clase original -0.00000286, Nueva clase: 1\n",
      "Palabra 'disfrutado': Cambio en probabilidad de clase original -0.00000286, Nueva clase: 1\n",
      "Palabra ',': Cambio en probabilidad de clase original -0.00000274, Nueva clase: 1\n",
      "Palabra 'o': Cambio en probabilidad de clase original -0.00000262, Nueva clase: 1\n",
      "Palabra 'aunque': Cambio en probabilidad de clase original -0.00000262, Nueva clase: 1\n",
      "Palabra 'he': Cambio en probabilidad de clase original -0.00000262, Nueva clase: 1\n",
      "Palabra 'el': Cambio en probabilidad de clase original -0.00000250, Nueva clase: 1\n",
      "Palabra 'disfrutar√°s': Cambio en probabilidad de clase original 0.00000250, Nueva clase: 1\n",
      "Palabra 'cine': Cambio en probabilidad de clase original 0.00000238, Nueva clase: 1\n",
      "Palabra 'aburrida': Cambio en probabilidad de clase original -0.00000238, Nueva clase: 1\n",
      "Palabra 'historia': Cambio en probabilidad de clase original 0.00000238, Nueva clase: 1\n",
      "Palabra 'como': Cambio en probabilidad de clase original 0.00000226, Nueva clase: 1\n",
      "Palabra ')': Cambio en probabilidad de clase original -0.00000226, Nueva clase: 1\n",
      "Palabra ')': Cambio en probabilidad de clase original -0.00000226, Nueva clase: 1\n",
      "Palabra 'lenta': Cambio en probabilidad de clase original -0.00000203, Nueva clase: 1\n",
      "Palabra 'Casino': Cambio en probabilidad de clase original -0.00000203, Nueva clase: 1\n",
      "Palabra ',': Cambio en probabilidad de clase original 0.00000179, Nueva clase: 1\n",
      "Palabra ',': Cambio en probabilidad de clase original -0.00000155, Nueva clase: 1\n",
      "Palabra 'de': Cambio en probabilidad de clase original 0.00000155, Nueva clase: 1\n",
      "Palabra 'divertida': Cambio en probabilidad de clase original -0.00000131, Nueva clase: 1\n",
      "Palabra 'visto': Cambio en probabilidad de clase original 0.00000131, Nueva clase: 1\n",
      "Palabra 'en': Cambio en probabilidad de clase original 0.00000131, Nueva clase: 1\n",
      "Palabra 'decepcionado': Cambio en probabilidad de clase original -0.00000119, Nueva clase: 1\n",
      "Palabra '.': Cambio en probabilidad de clase original 0.00000119, Nueva clase: 1\n",
      "Palabra 'The': Cambio en probabilidad de clase original -0.00000107, Nueva clase: 1\n",
      "Palabra 'entregas': Cambio en probabilidad de clase original -0.00000083, Nueva clase: 1\n",
      "Palabra 'on': Cambio en probabilidad de clase original -0.00000072, Nueva clase: 1\n",
      "Palabra 'y': Cambio en probabilidad de clase original -0.00000060, Nueva clase: 1\n",
      "Palabra 'Band': Cambio en probabilidad de clase original -0.00000060, Nueva clase: 1\n",
      "Palabra 'otras': Cambio en probabilidad de clase original -0.00000048, Nueva clase: 1\n",
      "Palabra '(': Cambio en probabilidad de clase original 0.00000036, Nueva clase: 1\n",
      "Palabra 'Saw': Cambio en probabilidad de clase original 0.00000024, Nueva clase: 1\n",
      "Palabra 'mucho': Cambio en probabilidad de clase original -0.00000024, Nueva clase: 1\n",
      "Palabra 'demasiado': Cambio en probabilidad de clase original 0.00000000, Nueva clase: 1\n",
      "Palabra 'Greatest': Cambio en probabilidad de clase original 0.00000000, Nueva clase: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "\n",
    "# Cargar el modelo en espa√±ol\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "frase = \"¬°Hola! Esto es un ejemplo con spaCy.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Cargar el modelo y el tokenizador preentrenados\n",
    "output_dir = nombredecarpetaatrabajar + \"/stopwords/\" + \"/model\"\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "loaded_model = BertForSequenceClassification.from_pretrained(output_dir)\n",
    "\n",
    "# Verificar si CUDA est√° disponible y mover el modelo a la GPU si es posible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()  # Colocar en modo evaluaci√≥n\n",
    "\n",
    "# Funci√≥n para obtener la predicci√≥n de una oraci√≥n\n",
    "def predict_sentiment(text):\n",
    "    inputs = loaded_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Mover los inputs a la GPU\n",
    "    with torch.no_grad():  # Desactivar el c√°lculo de gradientes\n",
    "        outputs = loaded_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    print(logits)\n",
    "    probabilities = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "    print(probabilities)\n",
    "     \n",
    "    return probabilities[0]\n",
    "\n",
    "# Funci√≥n para analizar el impacto de cada palabra en la clasificaci√≥n\n",
    "def analyze_word_importance(text):\n",
    "    # Obtener la predicci√≥n de la oraci√≥n original\n",
    "    original_probs = predict_sentiment(text)\n",
    "    original_class = np.argmax(original_probs)\n",
    "    print(f\"Texto original: '{text}'\")\n",
    "    print(f\"Predicci√≥n original: Clase {original_class}, Probabilidades {original_probs}\")\n",
    "\n",
    "    # Tokenizar el texto y convertirlo en tokens individuales\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    word_importance = []\n",
    "\n",
    "    # Perturbar el texto eliminando una palabra a la vez\n",
    "    for i, token in enumerate(tokens):\n",
    "        # Crear un texto nuevo sin el token actual\n",
    "        perturbed_tokens = tokens[:i] + tokens[i+1:]\n",
    "        perturbed_text = loaded_tokenizer.convert_tokens_to_string(perturbed_tokens)\n",
    "\n",
    "        # Obtener la predicci√≥n del texto perturbado\n",
    "        perturbed_probs = predict_sentiment(perturbed_text)\n",
    "        print(perturbed_probs)\n",
    "        perturbed_class = np.argmax(perturbed_probs)\n",
    "\n",
    "        # Calcular el cambio en la probabilidad para la clase original\n",
    "        prob_change = original_probs[original_class] - perturbed_probs[original_class]\n",
    "        \n",
    "        # Guardar el resultado\n",
    "        word_importance.append((token, prob_change, perturbed_class))\n",
    "        print(f\"Sin '{token}': Clase {perturbed_class}, Probabilidades {perturbed_probs}, Cambio en probabilidad de clase original: {prob_change}\")\n",
    "\n",
    "    # Ordenar las palabras por importancia en la probabilidad de la clase original\n",
    "    word_importance = sorted(word_importance, key=lambda x: abs(x[1]), reverse=True)\n",
    "    print(\"\\nPalabras ordenadas por impacto en la predicci√≥n de la clase original:\")\n",
    "    for token, change, new_class in word_importance:\n",
    "        print(f\"Palabra '{token}': Cambio en probabilidad de clase original {change:.8f}, Nueva clase: {new_class}\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "text_example = \"Originalmente, yo era fan√°tico de Tenacious D por su primer √°lbum y, naturalmente, escuch√© algunas canciones de The P.O.D., pero qued√© bastante decepcionado. Despu√©s de ver la pel√≠cula, mi perspectiva cambi√≥. La pel√≠cula es bastante divertida de principio a fin, y me encontr√© enganchado a pesar de que la trama era realmente absurda, gracias a las actitudes que KG y Jaybles retratan en la pel√≠cula. Mucho m√°s entretenida y agradable que otras pel√≠culas que he visto en el cine √∫ltimamente, como Saw III (aburrida y lenta) o Casino Royale (demasiado homoer√≥tica), aunque he disfrutado mucho entregas anteriores. Si disfrutaste Borat, seguramente disfrutar√°s la historia de The Greatest Band on Earth.\"\n",
    "analyze_word_importance(text_example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
